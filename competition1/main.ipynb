{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Competition 1 Predicting News Popularity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "import lightgbm as lgb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/huangmorris/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Some CONSTANTS\n",
        "RANDOM_STATE = 42\n",
        "INPUT_DIR = \"./input/\"\n",
        "OUTPUT_DIR = \"./output/\"\n",
        "MODEL_SAVE_DIR = \"./model_saves/\"\n",
        "\n",
        "# Create directories if not exist\n",
        "if not os.path.exists(INPUT_DIR):\n",
        "    os.makedirs(INPUT_DIR)\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "if not os.path.exists(MODEL_SAVE_DIR):\n",
        "    os.makedirs(MODEL_SAVE_DIR)\n",
        "\n",
        "# Download nltk stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOP = stopwords.words(\"english\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input & Output module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def input(chunksize: int = 1000, val_size: float = 0.2):\n",
        "    \"\"\"\n",
        "    讀取輸入數據，並返回一個生成器和一個測試數據集。\n",
        "\n",
        "    Args:\n",
        "        chunksize: int，每次讀取的 chunk 大小。默認為 1000。\n",
        "        val_size: float，驗證集的比例。默認為 0.2。\n",
        "\n",
        "    Returns:\n",
        "        stream: generator，用於進行 Out-of-Core learning 時，所使用的 stream generator。詳細參考教學 Notebook 的 Out-of-Core 環節。\n",
        "        testset: Dataclass: 包含了 test feature 和 ID。\n",
        "    \"\"\"\n",
        "\n",
        "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
        "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
        "\n",
        "    # 用於進行 Out-of-Core learning 時，所使用的 stream generator\n",
        "    def get_stream(train_path, chunksize, val_size=0.2):\n",
        "        @dataclass\n",
        "        class TrainValSet:\n",
        "            x_train: pd.Series\n",
        "            x_val: pd.Series\n",
        "            y_train: pd.Series\n",
        "            y_val: pd.Series\n",
        "\n",
        "        # 將資料依照 validation size 分成 train/validataion\n",
        "        for chunk in pd.read_csv(train_path, chunksize=chunksize):\n",
        "            train_val = train_test_split(\n",
        "                chunk[\"Page content\"],\n",
        "                chunk[\"Popularity\"],\n",
        "                test_size=val_size,\n",
        "                random_state=RANDOM_STATE,\n",
        "            )\n",
        "\n",
        "            # 重設 x_trian, x_val, y_train, y_val 的 index\n",
        "            train_val = [x.reset_index() for x in train_val]\n",
        "            yield TrainValSet(*train_val)\n",
        "\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    x_test = df_test[\"Page content\"]\n",
        "    id_test = df_test[\"Id\"]\n",
        "\n",
        "    @dataclass\n",
        "    class TestSet:\n",
        "        x: pd.Series\n",
        "        id: pd.Series\n",
        "\n",
        "    return get_stream(train_path, chunksize, val_size), TestSet(x_test, id_test)\n",
        "\n",
        "\n",
        "def output(id_test: pd.Series, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    將預測結果寫入 `OUTPUT_DIR` 資料夾中，並以當前時間命名\n",
        "\n",
        "    Args:\n",
        "        id_test: Pandas Series 包含 test data 的 id\n",
        "        y_pred: NumPy array 包含預測結果（0d/1d）\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    output_filename = f\"output_{timestamp}.csv\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
        "    output_df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Enignnering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning\n",
        "\n",
        "目前基本上是教學 Notebook 中的內容。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_cleaner(text: str):\n",
        "    \"\"\"\n",
        "    清理文本數據，去除 HTML 標籤和表情符號，並將文本轉換為小寫字母。\n",
        "\n",
        "    Args:\n",
        "        text: str，需要清理的文本數據。\n",
        "\n",
        "    Returns:\n",
        "        str，清理後的文本數據。\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
        "    r = r\"(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)\"\n",
        "    emoticons = re.findall(r, text)\n",
        "    text = re.sub(r, \"\", text)\n",
        "\n",
        "    # convert to lowercase and append all emoticons behind (with space in between)\n",
        "    # replace('-','') removes nose of emoticons\n",
        "    text = (\n",
        "        re.sub(r\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer_stem_nostop(text):\n",
        "    \"\"\"\n",
        "    對文本進行分詞和詞幹提取。\n",
        "\n",
        "    Args:\n",
        "        text: str，需要進行分詞和詞幹提取的文本數據。\n",
        "\n",
        "    Returns:\n",
        "        list，包含了文本數據中的詞幹。\n",
        "    \"\"\"\n",
        "\n",
        "    porter = PorterStemmer()\n",
        "    return [\n",
        "        porter.stem(w)\n",
        "        for w in word_tokenize(text, preserve_line=True)\n",
        "        if w not in STOP and re.match(\"[a-zA-Z]+\", w)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 調用函式\n",
        "\n",
        "用於更輕鬆的調用之後新增的比如：加新特徵或其他前處理的 Function。\n",
        "\n",
        "還有很多要調整。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FeatureParams:\n",
        "    hash_feature_num: int = 2**10\n",
        "\n",
        "\n",
        "def get_features(feature_params: FeatureParams, texts: pd.Series):\n",
        "    \"\"\"\n",
        "    對文本數據進行特徵工程，返回一包含轉換的基本特徵及生成的各種特徵的 DataFrame。\n",
        "\n",
        "    Args:\n",
        "        feature_params: FeatureParams，特徵參數。\n",
        "        texts: pd.Series，文本數據。\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # 基本的 HashingVectorizer 轉換\n",
        "    hash_vectorizer = HashingVectorizer(\n",
        "        n_features=feature_params.hash_feature_num,\n",
        "        preprocessor=text_cleaner,\n",
        "        tokenizer=tokenizer_stem_nostop,\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        hash_vectorizer.fit_transform(texts).toarray(),\n",
        "        columns=[\"v\" + str(i) for i in range(feature_params.hash_feature_num)],\n",
        "    )\n",
        "\n",
        "    # TODO: 增加額外的特徵工程\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Out-of-Core learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/huangmorris/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "stream, testset = input()\n",
        "dataset = next(stream)\n",
        "feature_params = FeatureParams(hash_feature_num=2**10)\n",
        "features_train = get_features(feature_params, dataset.x_train.loc[:10, \"Page content\"])\n",
        "features_val = get_features(feature_params, dataset.x_val.loc[:10, \"Page content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = os.path.join(MODEL_SAVE_DIR, \"model.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "lightgbm.sklearn.LGBMClassifier"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = lgb.sklearn.LGBMClassifier()\n",
        "model.fit(\n",
        "    features_train[\"vectorized_features\"].toarray(),\n",
        ")\n",
        "\n",
        "model.booster_.save_model(save_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL_Comp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
