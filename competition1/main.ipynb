{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 1 Predicting News Popularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/huangmorris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Some CONSTANTS\n",
    "RANDOM_STATE = 42\n",
    "INPUT_DIR = \"./input/\"\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "\n",
    "# Download nltk stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "STOP = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input():\n",
    "    \"\"\"\n",
    "    從 `INPUT_DIR` 讀取 input data 並回傳 Dataset 物件\n",
    "\n",
    "    Returns:\n",
    "        Dataset: 包含 train/val/test 的物件\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # 將資料依照 80/20 分成 train/validataion\n",
    "    train_dataset = train_test_split(\n",
    "        train_df[\"Page content\"],\n",
    "        train_df[\"Popularity\"],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    x_test = pd.DataFrame(test_df[\"Page content\"])\n",
    "    id_test = test_df[\"Id\"]\n",
    "\n",
    "    train_dataset = [pd.DataFrame(x.reset_index()) for x in train_dataset]\n",
    "\n",
    "    @dataclass\n",
    "    class Dataset:\n",
    "        x_train: pd.DataFrame\n",
    "        x_val: pd.DataFrame\n",
    "        y_train: pd.DataFrame\n",
    "        y_val: pd.DataFrame\n",
    "        x_test: pd.DataFrame\n",
    "        id_test: pd.Series\n",
    "\n",
    "    return Dataset(*train_dataset, x_test, id_test)\n",
    "\n",
    "\n",
    "def output(id_test: pd.Series, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    將預測結果寫入 `OUTPUT_DIR` 資料夾中，並以當前時間命名\n",
    "\n",
    "    Args:\n",
    "        id_test: Pandas Series 包含 test data 的 id\n",
    "        y_pred: NumPy array 包含預測結果（0d/1d）\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_filename = f\"output_{timestamp}.csv\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
    "    output_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enignnering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "目前基本上是教學 Notebook 中的內容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text: str):\n",
    "    \"\"\"\n",
    "    清理文本數據，去除 HTML 標籤和表情符號，並將文本轉換為小寫字母。\n",
    "\n",
    "    Args:\n",
    "        text: str，需要清理的文本數據。\n",
    "\n",
    "    Returns:\n",
    "        str，清理後的文本數據。\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = r\"(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)\"\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, \"\", text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = (\n",
    "        re.sub(r\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    \"\"\"\n",
    "    對文本進行分詞和詞幹提取。\n",
    "\n",
    "    Args:\n",
    "        text: str，需要進行分詞和詞幹提取的文本數據。\n",
    "\n",
    "    Returns:\n",
    "        list，包含了文本數據中的詞幹。\n",
    "    \"\"\"\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return [\n",
    "        porter.stem(w)\n",
    "        for w in word_tokenize(text, preserve_line=True)\n",
    "        if w not in STOP and re.match(\"[a-zA-Z]+\", w)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bow (Bag-Of-Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m count \u001b[39m=\u001b[39m CountVectorizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), preprocessor\u001b[39m=\u001b[39mtext_cleaner, tokenizer\u001b[39m=\u001b[39mtokenizer_stem_nostop\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m doc_bag \u001b[39m=\u001b[39m count\u001b[39m.\u001b[39;49mfit_transform(dataset\u001b[39m.\u001b[39;49mx_train\u001b[39m.\u001b[39;49mloc[:\u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mPage content\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m doc_bag\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m對文本進行分詞和詞幹提取。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m    list，包含了文本數據中的詞幹。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m porter \u001b[39m=\u001b[39m PorterStemmer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     porter\u001b[39m.\u001b[39mstem(w)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word_tokenize(text, preserve_line\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m STOP \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m[a-zA-Z]+\u001b[39m\u001b[39m\"\u001b[39m, w)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ]\n",
      "\u001b[1;32m/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m對文本進行分詞和詞幹提取。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m    list，包含了文本數據中的詞幹。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m porter \u001b[39m=\u001b[39m PorterStemmer()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     porter\u001b[39m.\u001b[39;49mstem(w)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word_tokenize(text, preserve_line\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m STOP \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m[a-zA-Z]+\u001b[39m\u001b[39m\"\u001b[39m, w)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangmorris/Projects/1121_Deep_Learning_Competition/competition1/main.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/nltk/stem/porter.py:672\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    670\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1b(stem)\n\u001b[1;32m    671\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1c(stem)\n\u001b[0;32m--> 672\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step2(stem)\n\u001b[1;32m    673\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step3(stem)\n\u001b[1;32m    674\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step4(stem)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/nltk/stem/porter.py:513\u001b[0m, in \u001b[0;36mPorterStemmer._step2\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMARTIN_EXTENSIONS:\n\u001b[1;32m    511\u001b[0m     rules\u001b[39m.\u001b[39mappend((\u001b[39m\"\u001b[39m\u001b[39mlogi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_positive_measure))\n\u001b[0;32m--> 513\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_rule_list(word, rules)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL_Comp/lib/python3.10/site-packages/nltk/stem/porter.py:266\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m         \u001b[39m# Don't try any further rules\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m word\n\u001b[0;32m--> 266\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39;49mendswith(suffix):\n\u001b[1;32m    267\u001b[0m     stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_suffix(word, suffix, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m     \u001b[39mif\u001b[39;00m condition \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m condition(stem):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(\n",
    "    ngram_range=(1, 1), preprocessor=text_cleaner, tokenizer=tokenizer_stem_nostop\n",
    ")\n",
    "doc_bag = count.fit_transform(dataset.x_train[\"Page content\"].values)\n",
    "doc_bag.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
