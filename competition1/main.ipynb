{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 1 Predicting News Popularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/huangmorris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Some CONSTANTS\n",
    "RANDOM_STATE = 42\n",
    "INPUT_DIR = \"./input/\"\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "\n",
    "# Download nltk stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "STOP = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input():\n",
    "    \"\"\"\n",
    "    從 `INPUT_DIR` 讀取 input data 並回傳 Dataset 物件\n",
    "\n",
    "    Returns:\n",
    "        Dataset: 包含 train/val/test 的物件\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # 將資料依照 80/20 分成 train/validataion\n",
    "    train_dataset = train_test_split(\n",
    "        train_df[\"Page content\"],\n",
    "        train_df[\"Popularity\"],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    x_test = pd.DataFrame(test_df[\"Page content\"])\n",
    "    id_test = test_df[\"Id\"]\n",
    "\n",
    "    train_dataset = [pd.DataFrame(x.reset_index()) for x in train_dataset]\n",
    "\n",
    "    @dataclass\n",
    "    class Dataset:\n",
    "        x_train: pd.DataFrame\n",
    "        x_val: pd.DataFrame\n",
    "        y_train: pd.DataFrame\n",
    "        y_val: pd.DataFrame\n",
    "        x_test: pd.DataFrame\n",
    "        id_test: pd.Series\n",
    "\n",
    "    return Dataset(*train_dataset, x_test, id_test)\n",
    "\n",
    "\n",
    "def output(id_test: pd.Series, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    將預測結果寫入 `OUTPUT_DIR` 資料夾中，並以當前時間命名\n",
    "\n",
    "    Args:\n",
    "        id_test: Pandas Series 包含 test data 的 id\n",
    "        y_pred: NumPy array 包含預測結果（0d/1d）\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_filename = f\"output_{timestamp}.csv\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
    "    output_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enignnering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "目前基本上是教學 Notebook 中的內容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text: str):\n",
    "    \"\"\"\n",
    "    清理文本數據，去除 HTML 標籤和表情符號，並將文本轉換為小寫字母。\n",
    "\n",
    "    Args:\n",
    "        text: str，需要清理的文本數據。\n",
    "\n",
    "    Returns:\n",
    "        str，清理後的文本數據。\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = r\"(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)\"\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, \"\", text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = (\n",
    "        re.sub(r\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    \"\"\"\n",
    "    對文本進行分詞和詞幹提取。\n",
    "\n",
    "    Args:\n",
    "        text: str，需要進行分詞和詞幹提取的文本數據。\n",
    "\n",
    "    Returns:\n",
    "        list，包含了文本數據中的詞幹。\n",
    "    \"\"\"\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return [\n",
    "        porter.stem(w)\n",
    "        for w in word_tokenize(text, preserve_line=True)\n",
    "        if w not in STOP and re.match(\"[a-zA-Z]+\", w)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bow (Bag-Of-Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(\n",
    "    ngram_range=(1, 1), preprocessor=text_cleaner, tokenizer=tokenizer_stem_nostop\n",
    ")\n",
    "doc_bag = count.fit_transform(dataset.x_train[\"Page content\"].values)\n",
    "doc_bag.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
