{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Competition 1 Predicting News Popularity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "import joblib\n",
        "import warnings\n",
        "import mmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    cross_validate,\n",
        "    HalvingRandomSearchCV,\n",
        ")\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfVectorizer,\n",
        "    HashingVectorizer,\n",
        ")\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/huangmorris/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Some CONSTANTS\n",
        "RANDOM_STATE = 42\n",
        "INPUT_DIR = \"./input/\"\n",
        "OUTPUT_DIR = \"./output/\"\n",
        "MODEL_SAVE_DIR = \"./model_saves/\"\n",
        "\n",
        "# Create directories if not exist\n",
        "if not os.path.exists(INPUT_DIR):\n",
        "    os.makedirs(INPUT_DIR)\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "if not os.path.exists(MODEL_SAVE_DIR):\n",
        "    os.makedirs(MODEL_SAVE_DIR)\n",
        "\n",
        "# Download nltk stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOP = stopwords.words(\"english\")\n",
        "\n",
        "# Stop warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input & Output module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def input(chunksize: int = 1000, val_size: float = 0.2, stream: bool = True):\n",
        "    \"\"\"\n",
        "    讀取輸入數據，並返回一個生成器和一個測試數據集。\n",
        "\n",
        "    Args:\n",
        "        chunksize: int，每次讀取的 chunk 大小。默認為 1000。\n",
        "        val_size: float，驗證集的比例。默認為 0.2。\n",
        "        stream: bool，是否使用 Out-of-Core learning。默認為 True。\n",
        "\n",
        "    Returns:\n",
        "        stream == True:\n",
        "            stream_generator: 當用於進行 Out-of-Core learning 時，所使用的 stream generator。\n",
        "                              詳細參考教學 Notebook 的 Out-of-Core 環節。\n",
        "            testset: Dataclass: 包含了 test feature 和 ID。\n",
        "\n",
        "        stream == False:\n",
        "            trainset: Dataclass: 包含了 x_train, val_train, y_train, y_val。\n",
        "            testset: Dataclass: 包含了 test feature 和 ID。\n",
        "    \"\"\"\n",
        "\n",
        "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
        "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
        "\n",
        "    chunksize = (\n",
        "        chunksize if stream else get_file_len(os.path.join(INPUT_DIR, \"train.csv\"))\n",
        "    )\n",
        "\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    x_test = df_test[\"Page content\"]\n",
        "    id_test = df_test[\"Id\"]\n",
        "\n",
        "    @dataclass\n",
        "    class TestSet:\n",
        "        x: pd.Series\n",
        "        id: pd.Series\n",
        "\n",
        "    return_item = (\n",
        "        (get_stream(train_path, chunksize, val_size), TestSet(x_test, id_test))\n",
        "        if stream\n",
        "        else (\n",
        "            next(get_stream(train_path, chunksize, val_size)),\n",
        "            TestSet(x_test, id_test),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return return_item\n",
        "\n",
        "\n",
        "# 用於進行 Out-of-Core learning 時，所使用的 stream generator\n",
        "def get_stream(train_path, chunksize, val_size=0.2):\n",
        "    @dataclass\n",
        "    class Dataset:\n",
        "        x: pd.Series\n",
        "        y: pd.Series\n",
        "\n",
        "    # 將資料依照 validation size 分成 train/validataion\n",
        "    for chunk in pd.read_csv(train_path, chunksize=chunksize):\n",
        "        x = chunk[\"Page content\"]\n",
        "        y = chunk[\"Popularity\"]\n",
        "\n",
        "        yield Dataset(x, y)\n",
        "\n",
        "\n",
        "def get_file_len(path):\n",
        "    # 用於得到檔案的行數\n",
        "    with open(path, \"rb\") as f:\n",
        "        buf = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "        lines = 0\n",
        "        while buf.readline():\n",
        "            lines += 1\n",
        "        buf.close()\n",
        "\n",
        "        # Remove column rows\n",
        "        return lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def output(id_test: pd.Series, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    將預測結果寫入 `OUTPUT_DIR` 資料夾中，並以當前時間命名\n",
        "\n",
        "    Args:\n",
        "        id_test: Pandas Series 包含 test data 的 id\n",
        "        y_pred: NumPy array 包含預測結果（0d/1d）\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    output_filename = f\"output_{timestamp}.csv\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
        "    output_df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Enignnering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning\n",
        "\n",
        "目前基本上是教學 Notebook 中的內容。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_cleaner(text: str):\n",
        "    \"\"\"\n",
        "    清理文本數據，去除 HTML 標籤和表情符號，並將文本轉換為小寫字母。\n",
        "\n",
        "    Args:\n",
        "        text: str，需要清理的文本數據。\n",
        "\n",
        "    Returns:\n",
        "        str，清理後的文本數據。\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
        "    r = r\"(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)\"\n",
        "    emoticons = re.findall(r, text)\n",
        "    text = re.sub(r, \"\", text)\n",
        "\n",
        "    # convert to lowercase and append all emoticons behind (with space in between)\n",
        "    # replace('-','') removes nose of emoticons\n",
        "    text = (\n",
        "        re.sub(r\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer_stem_nostop(text):\n",
        "    \"\"\"\n",
        "    對文本進行分詞和詞幹提取。\n",
        "\n",
        "    Args:\n",
        "        text: str，需要進行分詞和詞幹提取的文本數據。\n",
        "\n",
        "    Returns:\n",
        "        list，包含了文本數據中的詞幹。\n",
        "    \"\"\"\n",
        "\n",
        "    porter = PorterStemmer()\n",
        "    return [\n",
        "        porter.stem(w)\n",
        "        for w in word_tokenize(text, preserve_line=True)\n",
        "        if w not in STOP and re.match(\"[a-zA-Z]+\", w)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hash_vectorize(feature_params, texts):\n",
        "    # 基本的 HashingVectorizer 轉換\n",
        "    hash_vectorizer = HashingVectorizer(\n",
        "        n_features=feature_params.hash_feature_num,\n",
        "        preprocessor=text_cleaner,\n",
        "        tokenizer=tokenizer_stem_nostop,\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame(hash_vectorizer.fit_transform(texts).toarray())\n",
        "\n",
        "\n",
        "def get_title(texts: pd.Series):\n",
        "    texts = texts.apply(\n",
        "        lambda x: BeautifulSoup(x, \"html.parser\").body.h1.string.strip().lower()\n",
        "    ).rename(\"title\")\n",
        "\n",
        "    return pd.DataFrame(texts)\n",
        "\n",
        "\n",
        "def get_topic(texts: pd.Series):\n",
        "    def helper(text):\n",
        "        a_list = BeautifulSoup(text, \"html.parser\").footer.find_all(\"a\")\n",
        "        topics = [re.sub(\"\\s+\", \"-\", a.string.strip().lower()) for a in a_list]\n",
        "        return \" \".join(topics)\n",
        "\n",
        "    return pd.DataFrame(texts.apply(helper).rename(\"topic\"))\n",
        "\n",
        "\n",
        "def get_datetime(texts: pd.Series):\n",
        "    def helper(text):\n",
        "        try:\n",
        "            datetime_str = BeautifulSoup(text, \"html.parser\").time[\"datetime\"]\n",
        "        except:\n",
        "            datetime_str = \"Thu, 01 Jan 2014 00:00:00 +0000\"\n",
        "\n",
        "        datetime_obj = datetime.datetime.strptime(\n",
        "            datetime_str, \"%a, %d %b %Y %H:%M:%S %z\"\n",
        "        )\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"year\": datetime_obj.year,\n",
        "                \"month\": datetime_obj.month,\n",
        "                \"day\": datetime_obj.day,\n",
        "                \"hour\": datetime_obj.hour,\n",
        "                \"minute\": datetime_obj.minute,\n",
        "                \"second\": datetime_obj.second,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(texts.apply(helper))\n",
        "\n",
        "\n",
        "def vectorize_texts(df: pd.DataFrame, vec_idx: list, vectorizer):\n",
        "    additional_dfs = [\n",
        "        pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(df.loc[:, idx]))\n",
        "        for idx in vec_idx\n",
        "    ]\n",
        "\n",
        "    return pd.concat(additional_dfs, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 調用函式\n",
        "\n",
        "用於更輕鬆的調用之後新增的比如：加新特徵或其他前處理的 Function。\n",
        "\n",
        "還有很多要調整。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FeatureParams:\n",
        "    do_get_title: bool = True\n",
        "    do_get_topic: bool = True\n",
        "    do_get_datetime: bool = True\n",
        "\n",
        "    vectorizer: str = \"count\"\n",
        "\n",
        "\n",
        "def get_features(feature_params: FeatureParams, texts: pd.Series):\n",
        "    \"\"\"\n",
        "    對文本數據進行特徵工程，返回一包含轉換的基本特徵及生成的各種特徵的 DataFrame。\n",
        "\n",
        "    Args:\n",
        "        feature_params: FeatureParams，特徵參數。\n",
        "        texts: pd.Series，文本數據。\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    def add_feature(original_df, additional_df):\n",
        "        return (\n",
        "            additional_df\n",
        "            if type(original_df) != pd.DataFrame\n",
        "            else pd.concat([original_df, additional_df], axis=1)\n",
        "        )\n",
        "\n",
        "    df = None\n",
        "    vec_idx = []\n",
        "\n",
        "    # TODO: 增加額外的特徵工程\n",
        "\n",
        "    if feature_params.do_get_title:\n",
        "        df = add_feature(df, get_title(texts))\n",
        "        vec_idx += [\"title\"]\n",
        "\n",
        "    if feature_params.do_get_topic:\n",
        "        df = add_feature(df, get_topic(texts))\n",
        "        vec_idx += [\"topic\"]\n",
        "\n",
        "    if feature_params.do_get_datetime:\n",
        "        df = add_feature(df, get_datetime(texts))\n",
        "\n",
        "    if feature_params.vectorizer == \"count\" and len(vec_idx) != 0:\n",
        "        df = add_feature(\n",
        "            df,\n",
        "            vectorize_texts(\n",
        "                df, vec_idx, CountVectorizer(tokenizer=tokenizer_stem_nostop)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    df = df.drop(columns=vec_idx, inplace=False)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training(model, x_train, y_train):\n",
        "    cv_result = cross_validate(\n",
        "        model,\n",
        "        x_train.values,\n",
        "        y_train.values,\n",
        "        cv=10,\n",
        "        scoring=\"roc_auc\",\n",
        "        return_train_score=True,\n",
        "        return_estimator=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    best_indice = np.argmax(cv_result[\"test_score\"])\n",
        "    train_score = cv_result[\"train_score\"][best_indice]\n",
        "    val_score = cv_result[\"test_score\"][best_indice]\n",
        "\n",
        "    print(f\"Train score: {train_score:.4f}\")\n",
        "    print(f\"Validation score: {val_score:.4f}\")\n",
        "\n",
        "    return cv_result[\"estimator\"][best_indice]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "cb_params = {\n",
        "    \"n_estimators\": 300,\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"eval_metric\": \"AUC\",\n",
        "    \"random_seed\": RANDOM_STATE,\n",
        "    \"logging_level\": \"Silent\",\n",
        "    \"thread_count\": 4,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pipeline(Model, model_params, model_name, feature_params):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model_filename = f\"{model_name}_{timestamp}.joblib\"\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
        "\n",
        "    dataset, testset = input(stream=False)\n",
        "    dataset.x = get_features(feature_params, dataset.x)\n",
        "    dataset.y = dataset.y.replace(-1, 0)\n",
        "\n",
        "    testset.x = get_features(feature_params, testset.x)\n",
        "\n",
        "    clf = Model(**model_params)\n",
        "    clf = training(clf, dataset.x, dataset.y)\n",
        "\n",
        "    joblib.dump(clf, model_save_path)\n",
        "\n",
        "    y_pred = clf.predict(testset.x.values)\n",
        "\n",
        "    output(testset.id, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xgb_training(x_train, y_train):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model_filename = f\"xgb_{timestamp}.joblib\"\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
        "\n",
        "    params = {\n",
        "        \"n_estimators\": 300,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"colsample_bytree\": 0.3,\n",
        "        \"early_stopping_rounds\": 10,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "    }\n",
        "\n",
        "    params_distribution = {\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "        \"max_depth\": [3, 5, 7, 9, 11],\n",
        "        \"alpha\": [0, 1, 3, 5, 7, 9, 10],\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    cv = HalvingRandomSearchCV(\n",
        "        model,\n",
        "        params_distribution,\n",
        "        factor=3,\n",
        "        n_jobs=1,\n",
        "        scoring=\"roc_auc\",\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "    cv.fit(x_train, y_train)\n",
        "\n",
        "    print(f\"Best validation score: {cv.best_score_:.4f}\")\n",
        "\n",
        "    model = cv.best_estimator_\n",
        "    joblib.dump(model, model_save_path)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_params = FeatureParams(do_get_title=False, do_get_topic=True)\n",
        "\n",
        "dataset, testset = input(stream=False)\n",
        "dataset.x = get_features(feature_params, dataset.x)\n",
        "dataset.y = dataset.y.replace(-1, 0)\n",
        "testset.x = get_features(feature_params, testset.x)\n",
        "\n",
        "xgb_model = xgb_training(dataset.x.values, dataset.y.values)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL_Comp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
