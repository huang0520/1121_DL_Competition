{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Competition 1 Predicting News Popularity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "import joblib\n",
        "import warnings\n",
        "import mmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    cross_validate,\n",
        "    HalvingRandomSearchCV,\n",
        ")\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfVectorizer,\n",
        "    HashingVectorizer,\n",
        ")\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/matcha0714/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/matcha0714/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Some CONSTANTS\n",
        "RANDOM_STATE = 42\n",
        "INPUT_DIR = \"./input/\"\n",
        "OUTPUT_DIR = \"./output/\"\n",
        "MODEL_SAVE_DIR = \"./model_saves/\"\n",
        "\n",
        "# Create directories if not exist\n",
        "if not os.path.exists(INPUT_DIR):\n",
        "    os.makedirs(INPUT_DIR)\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "if not os.path.exists(MODEL_SAVE_DIR):\n",
        "    os.makedirs(MODEL_SAVE_DIR)\n",
        "\n",
        "# Download nltk stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOP = stopwords.words(\"english\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Stop warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input & Output module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def input(chunksize: int = 1000, val_size: float = 0.2, stream: bool = False):\n",
        "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
        "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
        "\n",
        "    chunksize = (\n",
        "        chunksize if stream else get_file_len(os.path.join(INPUT_DIR, \"train.csv\"))\n",
        "    )\n",
        "\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    x_test = df_test[\"Page content\"]\n",
        "    id_test = df_test[\"Id\"]\n",
        "\n",
        "    @dataclass\n",
        "    class TestSet:\n",
        "        x: pd.Series\n",
        "        id: pd.Series\n",
        "\n",
        "    return_item = (\n",
        "        (get_stream(train_path, chunksize, val_size), TestSet(x_test, id_test))\n",
        "        if stream\n",
        "        else (\n",
        "            next(get_stream(train_path, chunksize, val_size)),\n",
        "            TestSet(x_test, id_test),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return return_item\n",
        "\n",
        "\n",
        "# 用於進行 Out-of-Core learning 時，所使用的 stream generator\n",
        "def get_stream(train_path, chunksize, val_size=0.2):\n",
        "    @dataclass\n",
        "    class Dataset:\n",
        "        x: pd.Series\n",
        "        y: pd.Series\n",
        "\n",
        "    # 將資料依照 validation size 分成 train/validataion\n",
        "    for chunk in pd.read_csv(train_path, chunksize=chunksize):\n",
        "        x = chunk[\"Page content\"]\n",
        "        y = chunk[\"Popularity\"]\n",
        "\n",
        "        yield Dataset(x, y)\n",
        "\n",
        "\n",
        "def get_file_len(path):\n",
        "    # 用於得到檔案的行數\n",
        "    with open(path, \"rb\") as f:\n",
        "        buf = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "        lines = 0\n",
        "        while buf.readline():\n",
        "            lines += 1\n",
        "        buf.close()\n",
        "\n",
        "        # Remove column rows\n",
        "        return lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def output(id_test: pd.Series, y_pred: np.ndarray, info: str = None):\n",
        "    if info == None:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    else:\n",
        "        timestamp = info\n",
        "\n",
        "    output_filename = f\"output_{timestamp}.csv\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
        "    output_df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Enignnering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_cleaner(self, text: str):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
        "    r = r\"(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)\"\n",
        "    emoticons = re.findall(r, text)\n",
        "    text = re.sub(r, \"\", text)\n",
        "\n",
        "    # convert to lowercase and append all emoticons behind (with space in between)\n",
        "    # replace('-','') removes nose of emoticons\n",
        "    text = (\n",
        "        re.sub(r\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer(self, text):\n",
        "    text = re.sub(\"([\\w]+)'[\\w]+\", (lambda match_obj: match_obj.group(1)), text)\n",
        "    text = re.sub(\"\\.\", \"\", text)\n",
        "    text = re.sub(\"[^\\w]+\", \" \", text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(s) for s in re.split(\"\\s+\", text.strip())]\n",
        "\n",
        "\n",
        "def get_title(self, soup_texts):\n",
        "    return pd.DataFrame(\n",
        "        soup_texts.apply(lambda x: x.body.h1.string.strip().lower()).rename(\"title\")\n",
        "    )\n",
        "\n",
        "\n",
        "def get_topic(self, soup_texts):\n",
        "    def helper(text):\n",
        "        a_list = text.body.find(\"footer\", {\"class\": \"article-topics\"}).find_all(\"a\")\n",
        "        topics = [re.sub(\"\\s+\", \"-\", a.string.strip().lower()) for a in a_list]\n",
        "        return \" \".join(topics)\n",
        "\n",
        "    return pd.DataFrame(soup_texts.apply(helper).rename(\"topic\"))\n",
        "\n",
        "\n",
        "def get_datetime(self, soup_texts):\n",
        "    def helper(text):\n",
        "        try:\n",
        "            datetime_str = text.time[\"datetime\"]\n",
        "        except:\n",
        "            datetime_str = \"Thu, 01 Jan 2014 00:00:00 +0000\"\n",
        "\n",
        "        datetime_obj = datetime.datetime.strptime(\n",
        "            datetime_str, \"%a, %d %b %Y %H:%M:%S %z\"\n",
        "        )\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"year\": datetime_obj.year,\n",
        "                \"month\": datetime_obj.month,\n",
        "                \"day\": datetime_obj.day,\n",
        "                \"hour\": datetime_obj.hour,\n",
        "                \"minute\": datetime_obj.minute,\n",
        "                \"second\": datetime_obj.second,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(soup_texts.apply(helper))\n",
        "\n",
        "\n",
        "def get_content_length(self, soup_texts):\n",
        "    def helper(text):\n",
        "        content = text.find(\"section\", class_=\"article-content\").get_text()\n",
        "        return len(content)\n",
        "\n",
        "    return pd.DataFrame(soup_texts.apply(helper).rename(\"content_length\"))\n",
        "\n",
        "\n",
        "def vectorize_texts(self, df: pd.DataFrame, vec_idx: list, vectorizer):\n",
        "    additional_dfs = [\n",
        "        pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(df.loc[:, idx]))\n",
        "        for idx in vec_idx\n",
        "    ]\n",
        "\n",
        "    return pd.concat(additional_dfs, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 調用函式\n",
        "\n",
        "用於更輕鬆的調用之後新增的比如：加新特徵或其他前處理的 Function。\n",
        "\n",
        "還有很多要調整。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FeatureParams:\n",
        "    do_get_title: bool = True\n",
        "    do_get_topic: bool = True\n",
        "    do_get_datetime: bool = True\n",
        "    do_get_content_length: bool = True\n",
        "\n",
        "    vectorizer: str = \"count\"\n",
        "\n",
        "\n",
        "class FeaturePreprocessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        get_title: bool = True,\n",
        "        get_topic: bool = True,\n",
        "        get_datetime: bool = True,\n",
        "        get_content_length: bool = True,\n",
        "    ) -> None:\n",
        "        self.get_title = get_title\n",
        "        self.get_topic = get_topic\n",
        "        self.get_datetime = get_datetime\n",
        "        self.get_content_length = get_content_length\n",
        "\n",
        "    __text_cleaner = text_cleaner\n",
        "    __tokenizer = tokenizer\n",
        "    __get_title = get_title\n",
        "    __get_topic = get_topic\n",
        "    __get_datetime = get_datetime\n",
        "    __get_content_length = get_content_length\n",
        "\n",
        "    def __add_feature(self, original_df, additional_df):\n",
        "        return (\n",
        "            additional_df\n",
        "            if type(original_df) != pd.DataFrame\n",
        "            else pd.concat([original_df, additional_df], axis=1)\n",
        "        )\n",
        "\n",
        "    def __get_feature(self, texts):\n",
        "        df = None\n",
        "        vec_idx = []\n",
        "\n",
        "        soup_texts = texts.apply(lambda x: BeautifulSoup(x, \"html.parser\"))\n",
        "\n",
        "        if self.get_title:\n",
        "            df = self.__add_feature(df, self.__get_title(soup_texts))\n",
        "            vec_idx += [\"title\"]\n",
        "\n",
        "        if self.get_topic:\n",
        "            df = self.__add_feature(df, self.__get_topic(soup_texts))\n",
        "            vec_idx += [\"topic\"]\n",
        "\n",
        "        if self.get_datetime:\n",
        "            df = self.__add_feature(df, self.__get_datetime(soup_texts))\n",
        "\n",
        "        if self.get_content_length:\n",
        "            df = self.__add_feature(df, self.__get_content_length(soup_texts))\n",
        "\n",
        "        print(f\"Features: {df.columns.tolist()}\")\n",
        "\n",
        "        return df, vec_idx\n",
        "\n",
        "    def fit_transform(self, texts_train):\n",
        "        df, vec_idx = self.__get_feature(texts_train)\n",
        "\n",
        "        if len(vec_idx) != 0:\n",
        "            self.trans = ColumnTransformer(\n",
        "                [\n",
        "                    (\n",
        "                        idx,\n",
        "                        CountVectorizer(tokenizer=self.__tokenizer, lowercase=False),\n",
        "                        idx,\n",
        "                    )\n",
        "                    for idx in vec_idx\n",
        "                ],\n",
        "                remainder=\"drop\",\n",
        "                sparse_threshold=0.0,\n",
        "            )\n",
        "\n",
        "            addtional_df = pd.DataFrame(self.trans.fit_transform(df))\n",
        "            df = self.__add_feature(df, addtional_df)\n",
        "\n",
        "        return df.drop(columns=vec_idx, inplace=False)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        df, vec_idx = self.__get_feature(texts)\n",
        "\n",
        "        if len(vec_idx) != 0:\n",
        "            additional_df = pd.DataFrame(self.trans.transform(df))\n",
        "            df = self.__add_feature(df, additional_df)\n",
        "\n",
        "        return df.drop(columns=vec_idx, inplace=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: ['topic', 'year', 'month', 'day', 'hour', 'minute', 'second', 'content_length']\n",
            "Features: ['topic', 'year', 'month', 'day', 'hour', 'minute', 'second', 'content_length']\n"
          ]
        }
      ],
      "source": [
        "dataset, testset = input(stream=False)\n",
        "preprocessor = FeaturePreprocessor(get_title=False)\n",
        "\n",
        "dataset.x = preprocessor.fit_transform(dataset.x)\n",
        "testset.x = preprocessor.transform(testset.x)\n",
        "dataset.y = dataset.y.replace(-1, 0)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    dataset.x, dataset.y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "cat_features = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CatBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cb_trainer(x_train, y_train, x_val, y_val, params, cat_features):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    pool_train = cb.Pool(x_train, y_train, cat_features=cat_features)\n",
        "    pool_val = cb.Pool(x_val, y_val, cat_features=cat_features)\n",
        "\n",
        "    model = cb.to_classifier(cb.CatBoostClassifier(**params))\n",
        "    model.fit(pool_train, eval_set=pool_val, verbose=False, plot=True)\n",
        "\n",
        "    val_score = roc_auc_score(y_val, model.predict_proba(x_val)[:, 1])\n",
        "\n",
        "    model_filename = f\"cb_{timestamp}_{val_score*10000:.0f}.cbm\"\n",
        "    save_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
        "\n",
        "    print(f\"Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    model.save_model(save_path)\n",
        "\n",
        "    return model, timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27f6206c04cd48c1b8f5af708e4f7a8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation AUC: 0.5932\n"
          ]
        }
      ],
      "source": [
        "cb_params = {\n",
        "    \"random_strength\": 1.2,\n",
        "    \"border_count\": 254,\n",
        "    \"bootstrap_type\": \"MVS\",\n",
        "    \"mvs_reg\": 0.3,\n",
        "    \"eval_metric\": \"AUC\",\n",
        "    \"od_type\": \"IncToDec\",\n",
        "    \"od_pval\": 0.01,\n",
        "    \"iterations\": 2200,\n",
        "    \"loss_function\": \"CrossEntropy\",\n",
        "    \"l2_leaf_reg\": 3,\n",
        "    \"depth\": 10,\n",
        "    \"od_pval\": 0.01,\n",
        "    \"learning_rate\": 0.04,\n",
        "    \"random_seed\": RANDOM_STATE,\n",
        "    \"thread_count\": -1,\n",
        "}\n",
        "\n",
        "cb_model, cb_timestamp = cb_trainer(\n",
        "    x_train, y_train, x_val, y_val, cb_params, cat_features\n",
        ")\n",
        "output(testset.id, cb_model.predict_proba(testset.x)[:, 1], f\"cb_{cb_timestamp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xgb_training(x_train, y_train):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model_filename = f\"xgb_{timestamp}.joblib\"\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
        "\n",
        "    params = {\n",
        "        \"n_estimators\": 300,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"colsample_bytree\": 0.3,\n",
        "        \"early_stopping_rounds\": 10,\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "    }\n",
        "\n",
        "    params_distribution = {\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "        \"max_depth\": [3, 5, 7, 9, 11],\n",
        "        \"alpha\": [0, 1, 3, 5, 7, 9, 10],\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "\n",
        "    cv = HalvingRandomSearchCV(\n",
        "        model,\n",
        "        params_distribution,\n",
        "        factor=3,\n",
        "        n_jobs=1,\n",
        "        scoring=\"roc_auc\",\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "    cv.fit(x_train, y_train)\n",
        "\n",
        "    print(f\"Best validation score: {cv.best_score_:.4f}\")\n",
        "\n",
        "    model = cv.best_estimator_\n",
        "    joblib.dump(model, model_save_path)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LightGBM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lgb_trainer(x_train, y_train, x_val, y_val, params):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        eval_set=(x_val, y_val),\n",
        "        eval_metric=\"auc\",\n",
        "        callbacks=[lgb.early_stopping(1000)],\n",
        "    )\n",
        "\n",
        "    val_score = roc_auc_score(y_val, model.predict_proba(x_val)[:, 1])\n",
        "\n",
        "    model_filename = f\"lgb_{timestamp}_{val_score*10000:.0f}.txt\"\n",
        "    save_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
        "\n",
        "    print(f\"Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    model.booster_.save_model(save_path)\n",
        "\n",
        "    return model, timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 10916, number of negative: 11198\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3633\n",
            "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 1130\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493624 -> initscore=-0.025506\n",
            "[LightGBM] [Info] Start training from score -0.025506\n",
            "Training until validation scores don't improve for 1000 rounds\n",
            "Early stopping, best iteration is:\n",
            "[230]\tvalid_0's auc: 0.585885\tvalid_0's binary_logloss: 0.680961\n",
            "Validation AUC: 0.5859\n"
          ]
        }
      ],
      "source": [
        "lgb_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"n_estimators\": 3000,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"num_leaves\": 43,\n",
        "    \"min_child_samples\": 15,\n",
        "    \"min_child_weight\": 1e-3,\n",
        "    \"reg_alpha\": 0.1,\n",
        "    \"reg_lambda\": 0.1,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"n_jobs\": -1,\n",
        "}\n",
        "\n",
        "lgb_model, lgb_timestamp = lgb_trainer(x_train, y_train, x_val, y_val, lgb_params)\n",
        "output(testset.id, lgb_model.predict_proba(testset.x)[:, 1], f\"lgb_{lgb_timestamp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Voting\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL_Comp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
