{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 1 Predicting News Popularity\n",
    "\n",
    "Team Name: 窩不知道誒\n",
    "\n",
    "Team Members: 112501533 黃思誠 111062632 曾靖驊 112065527 劉承瑋\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import mmap\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optuna.integration.lightgbm import LightGBMTuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/matcha0714/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some CONSTANTS\n",
    "RANDOM_STATE = 42\n",
    "INPUT_DIR = \"./input/\"\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "MODEL_SAVE_DIR = \"./model_saves/\"\n",
    "\n",
    "# Create directories if not exist\n",
    "if not os.path.exists(INPUT_DIR):\n",
    "    os.makedirs(INPUT_DIR)\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "# Download nltk stopwords\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output module\n",
    "\n",
    "設定輸入及輸出的格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input():\n",
    "    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    @dataclass\n",
    "    class TestSet:\n",
    "        x: pd.Series\n",
    "        id: pd.Series\n",
    "\n",
    "    @dataclass\n",
    "    class Dataset:\n",
    "        x: pd.Series\n",
    "        y: pd.Series\n",
    "\n",
    "    dataset = Dataset(df_train[\"Page content\"], df_train[\"Popularity\"])\n",
    "    testset = TestSet(df_test[\"Page content\"], df_test[\"Id\"])\n",
    "\n",
    "    return dataset, testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(id_test: pd.Series, y_pred: np.ndarray, info: str = None):\n",
    "    if info == None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    else:\n",
    "        timestamp = info\n",
    "\n",
    "    output_filename = f\"output_{timestamp}.csv\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "    output_df = pd.DataFrame({\"Id\": id_test.ravel(), \"Popularity\": y_pred})\n",
    "    output_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enignnering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我們在這次 Competition 中所使用的 Data extraction 及 cleaning 的方式：\n",
    "\n",
    "- title: 文章標題\n",
    "- topic: 文章所屬的章節主題（footer 中的 article-topics）\n",
    "- channel: 文章所屬的頻道（article 中的 data-channel）\n",
    "- datatime: 文章發表的時間（年、月、日、時、分、秒），若有缺失值則用 \"Thu, 01 Jan 2014 00:00:00 +0000\" 填補，並且將文字如 Mon, Tue, etc. 及 Jan, Feb, etc. 轉換為數字\n",
    "- content_length: 文章的長度\n",
    "\n",
    "最後再對 Title, Topic, Channel 等文字特徵進行處理：\n",
    "\n",
    "1. 通過 Tokenizer 提取單字\n",
    "2. 通過 WordNetLemmatizer 還原單字原型\n",
    "3. 通過 CountVectorizer 計算單字的數量作為 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(r\"([\\w]+)'[\\w]+\", (lambda match_obj: match_obj.group(1)), text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w]+\", \" \", text)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(s) for s in re.split(r\"\\s+\", text.strip())]\n",
    "\n",
    "\n",
    "def get_title(soup_texts):\n",
    "    return pd.DataFrame(\n",
    "        soup_texts.apply(lambda x: x.body.h1.string.strip().lower()).rename(\"title\")\n",
    "    )\n",
    "\n",
    "\n",
    "def get_topic(soup_texts):\n",
    "    def helper(text):\n",
    "        a_list = text.body.find(\"footer\", {\"class\": \"article-topics\"}).find_all(\"a\")\n",
    "        topics = [re.sub(r\"\\s+\", \"-\", a.string.strip().lower()) for a in a_list]\n",
    "        return \" \".join(topics)\n",
    "\n",
    "    return pd.DataFrame(soup_texts.apply(helper).rename(\"topic\"))\n",
    "\n",
    "\n",
    "def get_datetime(soup_texts):\n",
    "    def helper(text):\n",
    "        try:\n",
    "            datetime_str = text.time[\"datetime\"]\n",
    "        except:\n",
    "            datetime_str = \"Thu, 01 Jan 2014 00:00:00 +0000\"\n",
    "\n",
    "        datetime_obj = datetime.datetime.strptime(\n",
    "            datetime_str, \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "        )\n",
    "\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"year\": datetime_obj.year,\n",
    "                \"month\": datetime_obj.month,\n",
    "                \"day\": datetime_obj.day,\n",
    "                \"hour\": datetime_obj.hour,\n",
    "                \"minute\": datetime_obj.minute,\n",
    "                \"second\": datetime_obj.second,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(soup_texts.apply(helper))\n",
    "\n",
    "\n",
    "def get_content_length(soup_texts):\n",
    "    def helper(text):\n",
    "        content = text.find(\"section\", class_=\"article-content\").get_text()\n",
    "        return len(content)\n",
    "\n",
    "    return pd.DataFrame(soup_texts.apply(helper).rename(\"content_length\"))\n",
    "\n",
    "\n",
    "def get_channel(soup_texts):\n",
    "    return pd.DataFrame(\n",
    "        soup_texts.apply(\n",
    "            lambda x: x.body.article[\"data-channel\"].strip().lower()\n",
    "        ).rename(\"channel\")\n",
    "    )\n",
    "\n",
    "\n",
    "def vectorize_texts(df: pd.DataFrame, vec_idx: list, vectorizer):\n",
    "    additional_dfs = [\n",
    "        pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(df.loc[:, idx]))\n",
    "        for idx in vec_idx\n",
    "    ]\n",
    "\n",
    "    return pd.concat(additional_dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "\n",
    "    def add_feature(self, original_df, additional_df):\n",
    "        return (\n",
    "            additional_df\n",
    "            if type(original_df) != pd.DataFrame\n",
    "            else pd.concat([original_df, additional_df], axis=1)\n",
    "        )\n",
    "\n",
    "    def preprocess(self, texts: pd.Series) -> pd.DataFrame:\n",
    "        soup_texts = texts.apply(\n",
    "            BeautifulSoup,\n",
    "            args=[\n",
    "                \"html.parser\",\n",
    "            ],\n",
    "        )\n",
    "        title = get_title(soup_texts)\n",
    "        topic = get_topic(soup_texts)\n",
    "        channel = get_channel(soup_texts)\n",
    "        datetime = get_datetime(soup_texts)\n",
    "        content_length = get_content_length(soup_texts)\n",
    "\n",
    "        return pd.concat([title, topic, datetime, channel, content_length], axis=1)\n",
    "\n",
    "    def fit_transform(self, texts: pd.Series):\n",
    "        df = self.preprocess(texts)\n",
    "\n",
    "        self.trans = ColumnTransformer(\n",
    "            [\n",
    "                (idx, CountVectorizer(tokenizer=tokenizer, lowercase=False), idx)\n",
    "                for idx in [\"title\", \"topic\", \"channel\"]\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            sparse_threshold=0.0,\n",
    "        )\n",
    "\n",
    "        trans_df = pd.DataFrame(self.trans.fit_transform(df))\n",
    "        df.drop([\"title\", \"topic\", \"channel\"], axis=1, inplace=True)\n",
    "\n",
    "        return pd.concat([df, trans_df], axis=1)\n",
    "\n",
    "    def transform(self, texts: pd.Series):\n",
    "        df = self.preprocess(texts)\n",
    "\n",
    "        trans_df = pd.DataFrame(self.trans.transform(df))\n",
    "        df.drop([\"title\", \"topic\", \"channel\"], axis=1, inplace=True)\n",
    "\n",
    "        return pd.concat([df, trans_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取 training, testing data，並進行上述的 Preprocess。再將 training data 切分為 train, validation set。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matcha0714/conda/envs/DL_Comp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load input data\n",
    "dataset, testset = input()\n",
    "\n",
    "preprocessor = PreProcessor()\n",
    "dataset.x = preprocessor.fit_transform(dataset.x)\n",
    "testset.x = preprocessor.transform(testset.x)\n",
    "dataset.y = dataset.y.replace(-1, 0)\n",
    "\n",
    "cat_features = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\"]\n",
    "\n",
    "# Split train and validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    dataset.x, dataset.y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, x_train, y_train, x_val, y_val, train_params={}):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    model.fit(x_train, y_train, **train_params)\n",
    "\n",
    "    train_score = roc_auc_score(y_train, model.predict_proba(x_train)[:, 1])\n",
    "    val_score = roc_auc_score(y_val, model.predict_proba(x_val)[:, 1])\n",
    "    print(f\"Train score: {train_score:.4f}\")\n",
    "    print(f\"Validation score: {val_score:.4f}\")\n",
    "\n",
    "    return model, timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "經過多次實驗後，選擇使用 CatBoost、LightGBM 作為最後 Voting classifier 的組合。原本還有嘗試 XGBoost，不過由於 Training 的時間稍長，加上表現不如另外兩者，因此最後不採納。\n",
    "\n",
    "- **CatBoost**\n",
    "\n",
    "Train score: 0.6567\n",
    "\n",
    "Validate score: 0.5934\n",
    "\n",
    "- **LightGBM**\n",
    "\n",
    "Train score: 0.6554\n",
    "\n",
    "Validate score: 0.5843\n",
    "\n",
    "- **Voting**\n",
    "\n",
    "最後將兩個模型的 Prediction 乘上權重並相加作為最終的 Prediction。\n",
    "\n",
    "而由於 CatBoost 在分數上的表現比 lightGBM 好不少，因此權重上就以 CatBoost 為主，lightGBM 主要是期待會降低 Over-fitting，並帶來一點分數上的提升。\n",
    "\n",
    "Weight = [0.9, 0.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7392\n",
      "Validation score: 0.5980\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    \"random_strength\": 1.2,\n",
    "    \"border_count\": 254,\n",
    "    \"bootstrap_type\": \"MVS\",\n",
    "    \"mvs_reg\": 0.3,\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"od_type\": \"IncToDec\",\n",
    "    \"od_pval\": 0.01,\n",
    "    \"loss_function\": \"CrossEntropy\",\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 10,\n",
    "    \"od_pval\": 0.01,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"random_seed\": RANDOM_STATE,\n",
    "    \"thread_count\": -1,\n",
    "    \"cat_features\": cat_features,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "cb_model = cb.CatBoostClassifier(\n",
    "    **cb_params,\n",
    "    iterations=1000,\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=200,\n",
    ")\n",
    "\n",
    "cb_model, cb_timestamp = trainer(cb_model, x_train, y_train, x_val, y_val, train_params)\n",
    "cb_iteration = cb_model.best_iteration_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's auc: 0.587338\tvalid_0's binary_logloss: 0.682014\n",
      "Train score: 0.6341\n",
      "Validation score: 0.5873\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"callbacks\": [lgb.early_stopping(100)],\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 10,\n",
    "    \"num_leaves\": 34,\n",
    "    \"min_child_samples\": 15,\n",
    "    \"subsample\": 0.8,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params, n_estimators=1000)\n",
    "\n",
    "lgb_model, lgb_timestamp = trainer(\n",
    "    lgb_model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    train_params,\n",
    ")\n",
    "lgb_iteration = lgb_model.best_iteration_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7386\n",
      "Validation score: 0.5994\n"
     ]
    }
   ],
   "source": [
    "weights = [0.9, 0.1]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"cb\", cb.CatBoostClassifier(**cb_params, iterations=cb_iteration)),\n",
    "        (\"lgb\", lgb.LGBMClassifier(**lgb_params, n_estimators=lgb_iteration)),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    weights=weights,\n",
    ")\n",
    "\n",
    "voting_clf, voting_timestamp = trainer(voting_clf, x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_clf.predict_proba(testset.x)[:, 1]\n",
    "output(testset.id, y_pred, voting_timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Public score: 0.58483 (7th)\n",
    "\n",
    "Private score: 0.60140 (3rd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "這次的 Competition 主要的難點是在對文字特徵的處理，尤其是對 html 的文字做處理。因為以前都沒有類似的經驗，因此除了想到有用的 Feature 之外，如何去 Parse 出這些 Feature 也是個挑戰。除此之外，Parse 完後的文字資料要如何處理也是個問題，實驗課提到的 Preprocessing 方式的表現不到非常好。雖然可能是後續沒調整好，但找到一個更好的如 Tokenize, Stemming 也花了不少時間。\n",
    "\n",
    "不過看原本 Public score 的排名我們原本不期待在 Private 會有多高的提升，沒想到直接上升到第 3 名，非常出乎我們的意料。而且在 Private 最好的成績的預測並不是在 Public 最好的，這點也令我們感到意外。看來這次的 Private 跟 Public 的資料分布上是有一定程度的落差的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
