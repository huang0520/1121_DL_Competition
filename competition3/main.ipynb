{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 14:30:07.900694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 14:30:07.900723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 14:30:07.900734: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_DIR: str = \"./data/dictionary\"\n",
    "DATASET_DIR: str = \"./data/dataset\"\n",
    "IMAGE_DIR: str = \"./data/102flowers\"\n",
    "CHECKPOINT_DIR: str = \"./checkpoints\"\n",
    "OUTPUT_DIR: str = \"./output\"\n",
    "\n",
    "# Tokenizer parameters\n",
    "MAX_SENTENCE_LENGTH: int = 20\n",
    "\n",
    "# Image parameters\n",
    "IMAGE_HEIGHT: int = 64\n",
    "IMAGE_WIDTH: int = 64\n",
    "IMAGE_CHANNELS: int = 3\n",
    "\n",
    "# Dataset parameters\n",
    "BATCH_SIZE: int = 64\n",
    "\n",
    "# Other parameters\n",
    "RANDOM_STATE: int = 42\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in [DICT_DIR, DATASET_DIR, IMAGE_DIR, CHECKPOINT_DIR, OUTPUT_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vocabularys: 5427\n",
      "ID of <PAD>: 5427, ID of <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "vocab_path: str = os.path.join(DICT_DIR, \"vocab.npy\")\n",
    "word2idx_path: str = os.path.join(DICT_DIR, \"word2Id.npy\")\n",
    "idx2word_path: str = os.path.join(DICT_DIR, \"id2Word.npy\")\n",
    "\n",
    "vocab: np.ndarray = np.load(vocab_path)\n",
    "print(f\"Total number of vocabularys: {len(vocab)}\")\n",
    "\n",
    "word2idx: dict = dict(np.load(word2idx_path))\n",
    "idx2word: dict = dict(np.load(idx2word_path))\n",
    "print(f\"ID of <PAD>: {word2idx['<PAD>']}, ID of <RARE>: {word2idx['<RARE>']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sentence2sequence(sent: str) -> list:\n",
    "    padding = 0\n",
    "\n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", sent.rstrip())\n",
    "    prep_line = prep_line.replace(\"-\", \" \")\n",
    "    prep_line = prep_line.replace(\"-\", \" \")\n",
    "    prep_line = prep_line.replace(\"  \", \" \")\n",
    "    prep_line = prep_line.replace(\".\", \"\")\n",
    "    tokens = prep_line.split(\" \")\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens)) if tokens[i] != \" \" and tokens[i] != \"\"\n",
    "    ]\n",
    "    token_len = len(tokens)\n",
    "    padding = MAX_SENTENCE_LENGTH - token_len\n",
    "\n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH,\n",
    "    # and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append(\"<PAD>\")\n",
    "\n",
    "    line = [\n",
    "        word2idx[tokens[k]] if tokens[k] in word2idx else word2idx[\"<RARE>\"]\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "test_sentence = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(test_sentence)\n",
    "print(sentence2sequence(test_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training data: 7370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: pd.DataFrame = pd.read_pickle(os.path.join(DATASET_DIR, \"text2ImgData.pkl\"))\n",
    "print(f\"Number of images in training data: {len(df)}\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self) -> None:\n",
    "        self.df_train: pd.DataFrame = pd.read_pickle(\n",
    "            os.path.join(DATASET_DIR, \"text2ImgData.pkl\")\n",
    "        )\n",
    "        self.df_test: pd.DataFrame = pd.read_pickle(\n",
    "            os.path.join(DATASET_DIR, \"testData.pkl\")\n",
    "        )\n",
    "\n",
    "    def _random_choose_caption(self, captions: np.ndarray) -> np.ndarray:\n",
    "        return np.array(list(map(random.choice, captions))).astype(np.int32)\n",
    "\n",
    "    def _load_image(self, path: tf.Tensor) -> np.ndarray:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=IMAGE_CHANNELS)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        return img\n",
    "\n",
    "    def _process_pipeline(self, img_path, caption):\n",
    "        img = self._load_image(img_path)\n",
    "        return img, caption\n",
    "\n",
    "    def generate_train(self) -> tf.data.Dataset:\n",
    "        captions = self.df_train[\"Captions\"].values\n",
    "        captions = self._random_choose_caption(captions)\n",
    "        image_paths = (\n",
    "            self.df_train[\"ImagePath\"]\n",
    "            .apply(lambda path: os.path.join(IMAGE_DIR, os.path.basename(path)))\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, captions))\n",
    "        dataset = (\n",
    "            dataset.map(self._process_pipeline, num_parallel_calls=AUTOTUNE)\n",
    "            .shuffle(len(image_paths))\n",
    "            .batch(BATCH_SIZE, drop_remainder=True)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def generate_test(self) -> tf.data.Dataset:\n",
    "        captions = self.df_test[\"Captions\"].values\n",
    "        captions = np.array([caption for caption in captions]).astype(np.int32)\n",
    "        idx = self.df_test[\"ID\"].values\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((captions, idx))\n",
    "        dataset = dataset.repeat().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)\n",
    "dataset_generator = DatasetGenerator()\n",
    "dataset_train = dataset_generator.generate_train()\n",
    "dataset_test = dataset_generator.generate_test()\n",
    "\n",
    "del dataset_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas[\"BATCH_SIZE\"]\n",
    "\n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.hparas[\"VOCAB_SIZE\"], self.hparas[\"EMBED_DIM\"]\n",
    "        )\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(\n",
    "            self.hparas[\"RNN_HIDDEN_SIZE\"],\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state=hidden)\n",
    "        return output[:, -1, :], state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas[\"BATCH_SIZE\"], self.hparas[\"RNN_HIDDEN_SIZE\"]))\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas[\"DENSE_DIM\"])\n",
    "        self.d2 = tf.keras.layers.Dense(64 * 64 * 3)\n",
    "\n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d2(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output\n",
    "\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas[\"DENSE_DIM\"])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas[\"DENSE_DIM\"])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "\n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "\n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "\n",
    "        return logits, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    \"MAX_SEQ_LENGTH\": 20,  # maximum sequence length\n",
    "    \"EMBED_DIM\": 256,  # word embedding dimension\n",
    "    \"VOCAB_SIZE\": len(word2idx),  # size of dictionary of captions\n",
    "    \"RNN_HIDDEN_SIZE\": 128,  # number of RNN neurons\n",
    "    \"Z_DIM\": 512,  # random noise z dimension\n",
    "    \"DENSE_DIM\": 128,  # number of neurons in dense layer\n",
    "    \"IMAGE_SIZE\": [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS],  # render image size\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LR\": 1e-4,\n",
    "    \"LR_DECAY\": 0.5,\n",
    "    \"BETA_1\": 0.5,\n",
    "    \"N_EPOCH\": 600,\n",
    "    \"N_SAMPLE\": len(dataset_train) * BATCH_SIZE,  # size of training data\n",
    "    \"PRINT_FREQ\": 1,  # printing frequency of loss\n",
    "}\n",
    "\n",
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_logit, fake_logit):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logit), real_logit)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logit), fake_logit)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas[\"LR\"])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas[\"LR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    text_encoder=text_encoder,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, CHECKPOINT_DIR, max_to_keep=5, checkpoint_name=\"ckpt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(\n",
    "        shape=[hparas[\"BATCH_SIZE\"], hparas[\"Z_DIM\"]], mean=0.0, stddev=1.0\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(grad_d, discriminator.trainable_variables)\n",
    "    )\n",
    "\n",
    "    return g_loss, d_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j * h : j * h + h, i * w : i * w + w, :] = image\n",
    "    return img\n",
    "\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size) * 0.5 + 0.5)\n",
    "\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "\n",
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(BATCH_SIZE)))\n",
    "sample_size = BATCH_SIZE\n",
    "sample_seed = np.random.normal(\n",
    "    loc=0.0, scale=1.0, size=(sample_size, hparas[\"Z_DIM\"])\n",
    ").astype(np.float32)\n",
    "sample_sentence = (\n",
    "    [\"the flower shown has yellow anther red pistil and bright red petals.\"]\n",
    "    * int(sample_size / ni)\n",
    "    + [\"this flower has petals that are yellow, white and purple and has dark lines\"]\n",
    "    * int(sample_size / ni)\n",
    "    + [\"the petals on this flower are white with a yellow center\"]\n",
    "    * int(sample_size / ni)\n",
    "    + [\"this flower has a lot of small round pink petals.\"] * int(sample_size / ni)\n",
    "    + [\"this flower is orange in color, and has petals that are ruffled and rounded.\"]\n",
    "    * int(sample_size / ni)\n",
    "    + [\"the flower has yellow petals and the center of it is brown.\"]\n",
    "    * int(sample_size / ni)\n",
    "    + [\"this flower has petals that are blue and white.\"] * int(sample_size / ni)\n",
    "    + [\n",
    "        \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "    ]\n",
    "    * int(sample_size / ni)\n",
    ")\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sentence2sequence(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(OUTPUT_DIR, \"train\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas[\"N_SAMPLE\"] / BATCH_SIZE)\n",
    "    pbar = trange(epochs, desc=\"Epoch\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for image, caption in dataset:\n",
    "            g_loss, d_loss = train_step(image, caption, hidden)\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "\n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"gen_loss\": g_total_loss.numpy() / steps_per_epoch,\n",
    "                \"disc_loss\": d_total_loss.numpy() / steps_per_epoch,\n",
    "                \"time\": time.time() - start,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            ckpt_manager.save()\n",
    "\n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas[\"PRINT_FREQ\"] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], f\"{output_dir}/train_{epoch:02d}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|████▎     | 257/600 [21:53<24:14,  4.24s/it, gen_loss=1.95, disc_loss=0.905, time=3.4]   "
     ]
    }
   ],
   "source": [
    "train(dataset_train, hparas[\"N_EPOCH\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(OUTPUT_DIR, \"inference\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = BATCH_SIZE\n",
    "    sample_seed = np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=(sample_size, hparas[\"Z_DIM\"])\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    start = time.time()\n",
    "    pbar = trange(819 // BATCH_SIZE + 1, desc=\"Inference\", unit=\"batch\")\n",
    "    for _ in pbar:\n",
    "        captions, idx = next(iter(dataset))\n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            plt.imsave(\n",
    "                f\"{output_dir}/inference_{idx[i]:04d}.jpg\",\n",
    "                fake_image[i].numpy() * 0.5 + 0.5,\n",
    "            )\n",
    "\n",
    "    print(\"Time for inference is {:.4f} sec\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 13/13 [00:02<00:00,  5.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 2.1868 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ckpt_manager.restore_or_initialize()\n",
    "inference(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matcha0714/local_home/1121_DL_Competition/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matcha0714.old/1121_DL_Competition/competition3/evaluation\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "--------------Evaluation Success-----------------\n",
      "/home/matcha0714.old/1121_DL_Competition/competition3\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "%cd evaluation\n",
    "!python inception_score.py ../output/inference ../output/score.csv 39\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9961 ± 0.1545\n"
     ]
    }
   ],
   "source": [
    "df_score = pd.read_csv(\"./output/score.csv\")\n",
    "print(f\"Score: {np.mean(df_score['score']):.4f} ± {np.std(df_score['score']):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
