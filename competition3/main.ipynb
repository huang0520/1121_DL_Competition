{"cells":[{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","## Prepare environment"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import shutil\n","from dataclasses import asdict, fields\n","from datetime import datetime, timedelta, timezone\n","from time import sleep\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wesley/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from icecream import ic, install\n","from sklearn.model_selection import train_test_split\n","from src.callback import EMACallback, PBarCallback, SamplePlotCallback\n","from src.config import (\n","    RANDOM_STATE,\n","    DatasetConfig,\n","    DirPath,\n","    ModelConfig,\n","    TrainConfig,\n","    export_config,\n",")\n","from src.dataset import generate_dataset\n","from src.model2 import DiffusionModel\n","from src.preprocess import (\n","    generate_embedding_df,\n","    generate_resize_image,\n","    generate_sample_embeddings,\n","    generate_unconditional_embeddings,\n",")\n","from src.utils import check_gpu\n","from tensorflow import keras\n","from tqdm import tqdm, trange\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1 Physical GPUs, 1 Logical GPUs\n"]}],"source":["check_gpu()\n","install()\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["for field in fields(DirPath):\n","    dir = getattr(DirPath, field.name)\n","    if not dir.exists():\n","        dir.mkdir(parents=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","## Preprocess data"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["if (DirPath.dataset / \"embeddings_train.pkl\").exists():\n","    df_train = pd.read_pickle(DirPath.dataset / \"embeddings_train.pkl\")\n","    df_test = pd.read_pickle(DirPath.dataset / \"embeddings_test.pkl\")\n","else:\n","    generate_embedding_df()\n","    df_train = pd.read_pickle(DirPath.dataset / \"embeddings_train.pkl\")\n","    df_test = pd.read_pickle(DirPath.dataset / \"embeddings_test.pkl\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if len(list((DirPath.resize_image).glob(\"*.jpg\"))) != len(\n","    list((DirPath.resize_image).glob(\"*.jpg\"))\n","):\n","    shutil.rmtree(DirPath.resize_image)\n","    (DirPath.resize_image).mkdir(parents=True)\n","    generate_resize_image()\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["sample_embeddings = generate_sample_embeddings()\n","unconditional_sample_embeddings = generate_unconditional_embeddings(10)\n","unconditional_test_embeddings = generate_unconditional_embeddings(\n","    TrainConfig.batch_size\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","## Dataset"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Captions</th>\n","      <th>Embeddings</th>\n","      <th>ImagePath</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1974</th>\n","      <td>[this small purple flower has several flat pet...</td>\n","      <td>[[[0.339286, 0.116460174, 0.10195106, 0.030953...</td>\n","      <td>data/102flowers/image_05827.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>5655</th>\n","      <td>[this flower is yellow in color with petals th...</td>\n","      <td>[[[0.339286, 0.116460174, 0.10195106, 0.030953...</td>\n","      <td>data/102flowers/image_00905.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Captions  \\\n","1974  [this small purple flower has several flat pet...   \n","5655  [this flower is yellow in color with petals th...   \n","\n","                                             Embeddings  \\\n","1974  [[[0.339286, 0.116460174, 0.10195106, 0.030953...   \n","5655  [[[0.339286, 0.116460174, 0.10195106, 0.030953...   \n","\n","                            ImagePath  \n","1974  data/102flowers/image_05827.jpg  \n","5655  data/102flowers/image_00905.jpg  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=RANDOM_STATE)\n","df_train.head(2)\n"]},{"cell_type":"markdown","metadata":{},"source":["%%<br>\n","train, val: (image, embedding) | test: (id, embedding)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["dataset_train = generate_dataset(df_train, \"train\", augment=True, method=\"all\")\n","dataset_val = generate_dataset(df_val, \"val\", augment=False, method=\"all\")\n","dataset_test = generate_dataset(df_test, \"test\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["dataset_test_size = len(df_test) // TrainConfig.batch_size + 1\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset size:\n","Train: 1174, Val: 293, Test: 18\n"]}],"source":["print(\"Dataset size:\")\n","print(\n","    f\"Train: {len(dataset_train)}, Val: {len(dataset_val)}, Test: {dataset_test_size}\"\n",")\n","# %% [markdown]\n","# ## Train\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["timestamp = datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y%m%d_%H%M%S\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["ckpt_path = DirPath.checkpoint / \"diffusion.ckpt\"\n","ckpt_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=ckpt_path,\n","    save_weights_only=True,\n","    save_best_only=True,\n","    monitor=\"val_image_loss\",\n","    verbose=0,\n",")\n","ema_callback = EMACallback(TrainConfig.ema)\n","plot_callback = SamplePlotCallback(\n","    sample_embeddings,\n","    unconditional_sample_embeddings,\n","    TrainConfig.plot_diffusion_steps,\n","    num_rows=2,\n","    num_cols=5,\n","    plot_frequency=5,\n","    cfg_scale=TrainConfig.cfg_scale,\n",")\n","pbar_callback = PBarCallback()\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["log_path = DirPath.log / f\"{timestamp}_loss.csv\"\n","params_path = DirPath.log / f\"{timestamp}_params.toml\"\n","csv_logger = keras.callbacks.CSVLogger(log_path, separator=\",\", append=False)\n","export_config(params_path)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Nomalizer adapting...\n"]},{"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n\t [[cond/then/_0/cond/sequential/random_rotation/stateful_uniform/Cast_1/_14]]\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n0 successful operations.\n0 derived errors ignored.\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n\t [[cond/then/_0/cond/sequential/random_rotation/stateful_uniform/Cast_1/_14]]\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n0 successful operations.\n0 derived errors ignored.\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_adapt_step_1186]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[1;32m/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.64/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNomalizer adapting...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.64/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m normalizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mNormalization()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.64/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m normalizer\u001b[39m.\u001b[39;49madapt(dataset_train\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m image, embedding: image))\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py:287\u001b[0m, in \u001b[0;36mNormalization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the mean and variance of values in a dataset.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[39m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/keras/src/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m--> 258\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[1;32m    259\u001b[0m         \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m    260\u001b[0m             context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\nDetected at node convert_image/Cast defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n\t [[cond/then/_0/cond/sequential/random_rotation/stateful_uniform/Cast_1/_14]]\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n0 successful operations.\n0 derived errors ignored.\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n\t [[cond/then/_0/cond/sequential/random_rotation/stateful_uniform/Cast_1/_14]]\n  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[500,703,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node convert_image/Cast}}]]\n0 successful operations.\n0 derived errors ignored.\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_adapt_step_1186]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["print(\"Nomalizer adapting...\")\n","normalizer = tf.keras.layers.Normalization()\n","normalizer.adapt(dataset_train.map(lambda image, embedding: image))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = DiffusionModel(**asdict(ModelConfig()))\n","model.compile(\n","    prediction_type=\"velocity\",\n","    normalizer=normalizer,\n","    optimizer=keras.optimizers.Lion(\n","        learning_rate=TrainConfig.lr_init,\n","        weight_decay=TrainConfig.weight_decay,\n","    ),\n","    loss=keras.losses.mean_absolute_error,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if TrainConfig.transfer:\n","    model.load_weights(ckpt_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Start training...\")\n","model.fit(\n","    dataset_train,\n","    validation_data=dataset_val,\n","    epochs=TrainConfig.epochs,\n","    verbose=0,\n","    callbacks=[\n","        ckpt_callback,\n","        csv_logger,\n","        pbar_callback,\n","        ema_callback,\n","        plot_callback,\n","    ],\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log = pd.read_csv(log_path)\n","train_image_loss = log[\"image_loss\"]\n","train_noise_loss = log[\"noise_loss\"]\n","val_image_loss = log[\"val_image_loss\"]\n","val_noise_loss = log[\"val_noise_loss\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(train_image_loss, label=\"train_image_loss\", color=\"blue\")\n","plt.plot(val_image_loss, label=\"val_image_loss\", linestyle=\"--\", color=\"blue\")\n","plt.plot(train_noise_loss, label=\"train_noise_loss\", color=\"orange\")\n","plt.plot(val_noise_loss, label=\"val_noise_loss\", linestyle=\"--\", color=\"orange\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Load best model...\")\n","model.load_weights(ckpt_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if (DirPath.output / \"inference\").exists():\n","    shutil.rmtree(DirPath.output / \"inference\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(DirPath.output / \"inference\").mkdir(parents=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_epoch = len(df_test) // TrainConfig.batch_size + 1\n","step = 0\n","for id, text_embeddings in tqdm(\n","    dataset_test, total=test_epoch, desc=\"Generate image:\", colour=\"green\"\n","):\n","    step += 1\n","    if step > test_epoch:\n","        break\n","    generated_images = model.generate(\n","        num_images=TrainConfig.batch_size,\n","        text_embs=text_embeddings,\n","        un_text_embs=unconditional_test_embeddings,\n","        diffusion_steps=TrainConfig.plot_diffusion_steps,\n","        cfg_scale=TrainConfig.cfg_scale,\n","    )\n","    for i, img in enumerate(generated_images):\n","        plt.imsave(\n","            DirPath.output / f\"inference/inference_{id[i]:04d}.jpg\",\n","            img.numpy(),\n","            vmin=0.0,\n","            vmax=1.0,\n","        )\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.chdir(\"./evaluation\")\n","os.system(\"python inception_score.py ../output/inference ../output/score.csv 39\")\n","os.chdir(\"..\")\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_score = pd.read_csv(\"./output/score.csv\")\n","print(f\"Score: {np.mean(df_score['score']):.4f} ± {np.std(df_score['score']):.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# %%"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
