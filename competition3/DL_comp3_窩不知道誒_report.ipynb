{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Competition 3: Reverse Image Caption\n","\n","Team name: 窩不知道誒\n","\n","Team Members: 112501533 黃思誠 112065527 劉承瑋  \n","\n","## Prepare environment"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import shutil\n","from dataclasses import asdict, fields\n","from datetime import datetime, timedelta, timezone\n","from time import sleep\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-17 19:50:28.283969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-17 19:50:29.300755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-17 19:50:29.300859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-17 19:50:29.436889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-17 19:50:29.752579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-17 19:50:32.219058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from icecream import ic, install\n","from sklearn.model_selection import train_test_split\n","# from src.callback import EMACallback, PBarCallback, SamplePlotCallback\n","# from src.config import (\n","#     RANDOM_STATE,\n","#     DatasetConfig,\n","#     DirPath,\n","#     ModelConfig,\n","#     TrainConfig,\n","#     export_config,\n","# )\n","# from src.dataset import generate_dataset\n","# from src.model2 import DiffusionModel\n","# from src.preprocess import (\n","#     generate_embedding_df,\n","#     generate_resize_image,\n","#     generate_sample_embeddings,\n","#     generate_unconditional_embeddings,\n","# )\n","# from src.utils import check_gpu\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tqdm import tqdm, trange\n","from pathlib import Path\n","from dataclasses import asdict, dataclass\n","import math\n","import toml\n","import cv2\n","import torch\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Physical devices cannot be modified after being initialized\n"]}],"source":["def check_gpu():\n","    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n","    if gpus:\n","        try:\n","            # Restrict TensorFlow to only use the first GPU\n","            tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n","\n","            # Currently, memory growth needs to be the same across GPUs\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n","            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","        except RuntimeError as e:\n","            # Memory growth must be set before GPUs have been initialized\n","            print(e)\n","check_gpu()\n","install()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Configuration"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","RANDOM_STATE: int = 0\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","RNG_GENERATOR: np.random.Generator = np.random.default_rng(RANDOM_STATE)\n","\n","\n","@dataclass\n","class DirPath:\n","    data: Path = Path(\"./data\")\n","    dict: Path = Path(\"./data/dictionary\")\n","    dataset: Path = Path(\"./data/dataset\")\n","    original_image: Path = Path(\"./data/102flowers\")\n","    resize_image: Path = Path(\"./data/resize_image\")\n","    checkpoint: Path = Path(\"./checkpoints\")\n","    output: Path = Path(\"./output\")\n","    log: Path = Path(\"./logs\")\n","\n","\n","@dataclass\n","class DatasetConfig:\n","    aug_prob: float = 0.3\n","    max_seq_len: int = 20\n","\n","\n","@dataclass\n","class ModelConfig:\n","    image_size: int = 64\n","    noise_embedding_dim: int = 512\n","    image_embedding_dim: int = 64\n","    text_embedding_shape: tuple[int] = (DatasetConfig.max_seq_len, 512)\n","    widths: tuple[int] = (64, 96, 128, 160)\n","    block_depth: int = 2\n","    embedding_max_frequency: float = 1000.0\n","    start_log_snr: float = 2.5\n","    end_log_snr: float = -7.5\n","\n","\n","@dataclass\n","class TrainConfig:\n","    batch_size: int = 48\n","    epochs: int = 50\n","    lr: float = 1e-4\n","    lr_init: float = 5e-5\n","    weight_decay: float = 5e-4\n","    ema: float = 0.999\n","    plot_diffusion_steps: int = 100\n","    transfer: bool = True\n","    cfg_scale: float = 3.6\n","\n","\n","def export_config(path: Path):\n","    output_dict = {\n","        \"DatasetConfig\": asdict(DatasetConfig()),\n","        \"ModelConfig\": asdict(ModelConfig()),\n","        \"TrainConfig\": asdict(TrainConfig()),\n","    }\n","\n","    with path.open(\"w\") as f_write:\n","        toml.dump(output_dict, f_write)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["for field in fields(DirPath):\n","    dir = getattr(DirPath, field.name)\n","    if not dir.exists():\n","        dir.mkdir(parents=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess data"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wesley/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import CLIPProcessor, CLIPTextModel\n","\n","TextTokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","TextEncoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def generate_embedding_df():\n","    word2_idx_path = DirPath.dict / \"word2Id.npy\"\n","    idx2_word_path = DirPath.dict / \"id2Word.npy\"\n","\n","    word2idx = dict(np.load(word2_idx_path))\n","    idx2word = dict(np.load(idx2_word_path))\n","\n","    def seq2sent(seq: list[int]) -> str:\n","        pad_idx = word2idx[\"<PAD>\"]\n","        sent = [idx2word[idx] for idx in seq if idx != pad_idx]\n","        return \" \".join(sent)\n","\n","    def embed_sent_list(sents: list[str]):\n","        tokens = TextTokenizer(\n","            sents,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\",\n","            max_length=DatasetConfig.max_seq_len,\n","        )\n","        position_ids = torch.arange(\n","            0, DatasetConfig.max_seq_len, device=tokens.input_ids.device\n","        )\n","        tokens.position_ids = position_ids\n","\n","        embeddings = TextEncoder(**tokens).last_hidden_state.detach().numpy()\n","        return embeddings\n","\n","    def generate_embedding_df(df_train, df_test):\n","        # Create embedding for training data\n","        tqdm.pandas(desc=\"Embedding training data\", colour=\"green\")\n","        cap_seqs = df_train[\"Captions\"].to_numpy()\n","        cap_sents = [\n","            [seq2sent(cap_seq) for cap_seq in _cap_seqs] for _cap_seqs in cap_seqs\n","        ]\n","        embeddings = pd.Series(cap_sents).progress_apply(embed_sent_list).to_numpy()\n","\n","        # Change image path\n","        image_paths = (\n","            df_train[\"ImagePath\"]\n","            .apply(lambda x: DirPath.resize_image / Path(x).name)\n","            .to_numpy()\n","        )\n","        df_train = pd.DataFrame({\n","            \"Captions\": cap_sents,\n","            \"Embeddings\": embeddings,\n","            \"ImagePath\": image_paths,\n","        })\n","\n","        # Create embedding for testing data\n","        tqdm.pandas(desc=\"Embedding testing data\", colour=\"green\")\n","        cap_seqs = df_test[\"Captions\"]\n","        cap_sents = [seq2sent(cap_seq) for cap_seq in cap_seqs]\n","        embeddings = pd.Series(cap_sents).progress_apply(embed_sent_list).to_numpy()\n","        id = df_test[\"ID\"].to_numpy()\n","        df_test = pd.DataFrame({\n","            \"Captions\": cap_sents,\n","            \"Embeddings\": embeddings,\n","            \"ID\": id,\n","        })\n","\n","        return df_train, df_test\n","\n","    embedding_train_path = DirPath.dataset / \"embeddings_train.pkl\"\n","    embedding_test_path = DirPath.dataset / \"embeddings_test.pkl\"\n","\n","    df_train = pd.read_pickle(DirPath.dataset / \"text2ImgData.pkl\")\n","    df_test = pd.read_pickle(DirPath.dataset / \"testData.pkl\")\n","    df_train, df_test = generate_embedding_df(df_train, df_test)\n","    df_train.to_pickle(embedding_train_path)\n","    df_test.to_pickle(embedding_test_path)\n","\n","\n","def generate_resize_image():\n","    image_paths: list[Path] = list(DirPath.original_image.glob(\"*.jpg\"))\n","\n","    for image_path in tqdm(image_paths, desc=\"Resize image\", colour=\"green\"):\n","        image = cv2.imread(str(image_path))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        height = image.shape[0]\n","        width = image.shape[1]\n","        crop_size = min(height, width)\n","        height_margin = (height - crop_size) // 2\n","        width_margin = (width - crop_size) // 2\n","\n","        image = image[\n","            height_margin : height_margin + crop_size,\n","            width_margin : width_margin + crop_size,\n","        ]\n","\n","        image = cv2.resize(image, (ModelConfig.image_size, ModelConfig.image_size))\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","        cv2.imwrite(str(DirPath.resize_image / image_path.name), image)\n","\n","\n","def generate_sample_embeddings():\n","    sample_sents = pd.read_csv(DirPath.data / \"sample_sentence.csv\")[\n","        \"sentence\"\n","    ].tolist()\n","    token = TextTokenizer(\n","        sample_sents,\n","        max_length=DatasetConfig.max_seq_len,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=\"max_length\",\n","    )\n","    sample_embeddings = TextEncoder(**token).last_hidden_state.detach().numpy()\n","\n","    return sample_embeddings\n","\n","\n","def generate_unconditional_embeddings(batch_size: int):\n","    uncoditional_token = TextTokenizer(\n","        \"\",\n","        max_length=DatasetConfig.max_seq_len,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=\"max_length\",\n","    )\n","    unconditional_embedding = (\n","        TextEncoder(**uncoditional_token).last_hidden_state.detach().numpy()\n","    )\n","    unconditional_embeddings = np.repeat(unconditional_embedding, batch_size, axis=0)\n","\n","    return unconditional_embeddings\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if (DirPath.dataset / \"embeddings_train.pkl\").exists():\n","    df_train = pd.read_pickle(DirPath.dataset / \"embeddings_train.pkl\")\n","    df_test = pd.read_pickle(DirPath.dataset / \"embeddings_test.pkl\")\n","else:\n","    generate_embedding_df()\n","    df_train = pd.read_pickle(DirPath.dataset / \"embeddings_train.pkl\")\n","    df_test = pd.read_pickle(DirPath.dataset / \"embeddings_test.pkl\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["if len(list((DirPath.resize_image).glob(\"*.jpg\"))) != len(\n","    list((DirPath.resize_image).glob(\"*.jpg\"))\n","):\n","    shutil.rmtree(DirPath.resize_image)\n","    (DirPath.resize_image).mkdir(parents=True)\n","    generate_resize_image()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["sample_embeddings = generate_sample_embeddings()\n","unconditional_sample_embeddings = generate_unconditional_embeddings(10)\n","unconditional_test_embeddings = generate_unconditional_embeddings(\n","    TrainConfig.batch_size\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Captions</th>\n","      <th>Embeddings</th>\n","      <th>ImagePath</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1974</th>\n","      <td>[this small purple flower has several flat pet...</td>\n","      <td>[[[0.339286, 0.116460174, 0.10195106, 0.030953...</td>\n","      <td>data/102flowers/image_05827.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>5655</th>\n","      <td>[this flower is yellow in color with petals th...</td>\n","      <td>[[[0.339286, 0.116460174, 0.10195106, 0.030953...</td>\n","      <td>data/102flowers/image_00905.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Captions  \\\n","1974  [this small purple flower has several flat pet...   \n","5655  [this flower is yellow in color with petals th...   \n","\n","                                             Embeddings  \\\n","1974  [[[0.339286, 0.116460174, 0.10195106, 0.030953...   \n","5655  [[[0.339286, 0.116460174, 0.10195106, 0.030953...   \n","\n","                            ImagePath  \n","1974  data/102flowers/image_05827.jpg  \n","5655  data/102flowers/image_00905.jpg  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=RANDOM_STATE)\n","df_train.head(2)\n"]},{"cell_type":"markdown","metadata":{},"source":["train, val: (image, embedding) | test: (id, embedding)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["\n","def load_image(path: tf.Tensor) -> tf.Tensor:\n","    image = tf.io.read_file(path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    return image\n","\n","\n","def augment_image(image: tf.Tensor, augmenter) -> tf.Tensor:\n","    apply = tf.random.uniform((), seed=RANDOM_STATE) < DatasetConfig.aug_prob\n","    image = augmenter(image) if apply else image\n","    return image\n","\n","def generate_dataset(\n","    df: pd.DataFrame, type: str, method: str = \"random\", augment: bool = True\n",") -> tf.data.Dataset:\n","    assert type in [\"train\", \"val\", \"test\"]\n","    assert method in [\"random\", \"all\"]\n","\n","    augmenter = tf.keras.Sequential([\n","        tf.keras.layers.experimental.preprocessing.RandomRotation(1, seed=RANDOM_STATE),\n","        tf.keras.layers.experimental.preprocessing.RandomFlip(\n","            \"horizontal_and_vertical\", seed=RANDOM_STATE\n","        ),\n","    ])\n","\n","    def map_fn(path, embedding):\n","        image = load_image(path)\n","        image = augment_image(image, augmenter) if augment else image\n","        return image, embedding\n","\n","    if method == \"random\":\n","        df_new = df\n","        embeddings = np.array([\n","            RNG_GENERATOR.choice(_embeddings, size=1).squeeze()\n","            for _embeddings in df[\"Embeddings\"]\n","        ])\n","    elif method == \"all\":\n","        df_new = df.explode(\"Embeddings\") if type != \"test\" else df\n","        embeddings = df_new[\"Embeddings\"].to_numpy()\n","        embeddings = np.stack(embeddings)\n","\n","    if type == \"train\" or type == \"val\":\n","        img_paths = df_new[\"ImagePath\"].to_numpy().astype(str)\n","\n","        dataset = tf.data.Dataset.from_tensor_slices((img_paths, embeddings))\n","        dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)\n","        dataset = (\n","            dataset.shuffle(len(embeddings), seed=RANDOM_STATE)\n","            if type == \"train\"\n","            else dataset\n","        )\n","        dataset = dataset.batch(TrainConfig.batch_size, drop_remainder=True).prefetch(\n","            AUTOTUNE\n","        )\n","\n","    elif type == \"test\":\n","        id = df[\"ID\"].to_numpy()\n","        dataset = tf.data.Dataset.from_tensor_slices((id, embeddings))\n","        dataset = dataset.repeat().batch(TrainConfig.batch_size).prefetch(AUTOTUNE)\n","\n","    return dataset\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["dataset_train = generate_dataset(df_train, \"train\", augment=True, method=\"all\")\n","dataset_val = generate_dataset(df_val, \"val\", augment=False, method=\"all\")\n","dataset_test = generate_dataset(df_test, \"test\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["dataset_test_size = len(df_test) // TrainConfig.batch_size + 1\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset size:\n","Train: 1174, Val: 293, Test: 18\n"]}],"source":["print(\"Dataset size:\")\n","print(\n","    f\"Train: {len(dataset_train)}, Val: {len(dataset_val)}, Test: {dataset_test_size}\"\n",")\n","# %% [markdown]\n","# ## Train\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["timestamp = datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y%m%d_%H%M%S\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["\n","class KID(keras.metrics.Metric):\n","    def __init__(self, name=\"KID\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.kid_record = tf.keras.metrics.Mean(name=\"kid_record\")\n","        self.encoder = keras.Sequential(\n","            [\n","                layers.Input(shape=(ModelConfig.image_size, ModelConfig.image_size, 3)),\n","                layers.Lambda(self.preprocess_input),\n","                keras.applications.InceptionV3(\n","                    include_top=False,\n","                    weights=\"imagenet\",\n","                    input_shape=(\n","                        ModelConfig.kid_image_size,\n","                        ModelConfig.kid_image_size,\n","                        3,\n","                    ),\n","                ),\n","                layers.GlobalAveragePooling2D(),\n","            ],\n","            name=\"inception_encoder\",\n","        )\n","\n","    def preprocess_input(self, images):\n","        images = tf.image.resize(\n","            images,\n","            (ModelConfig.kid_image_size, ModelConfig.kid_image_size),\n","            method=\"bicubic\",\n","            antialias=True,\n","        )\n","        images = tf.clip_by_value(images, 0.0, 1.0)\n","        images = keras.applications.inception_v3.preprocess_input(images * 255.0)\n","        return images\n","\n","    def polynomial_kernel(self, x, y):\n","        feature_dim = tf.cast(x.shape[1], tf.float32)\n","        return (tf.matmul(x, y, transpose_b=True) / feature_dim + 1) ** 3\n","\n","    def update_state(self, img_real, img_pred, sample_weight=None):\n","        feature_real = self.encoder(img_real)\n","        feature_pred = self.encoder(img_pred)\n","\n","        kernel_real = self.polynomial_kernel(feature_real, feature_real)\n","        kernel_pred = self.polynomial_kernel(feature_pred, feature_pred)\n","        kernel_cross = self.polynomial_kernel(feature_real, feature_pred)\n","\n","        mask = 1.0 - tf.eye(img_real.shape[0])\n","        batch_size = tf.cast(img_real.shape[0], tf.float32)\n","\n","        kid = (\n","            tf.reduce_sum(kernel_real * mask)\n","            + tf.reduce_sum(kernel_pred * mask)\n","            - 2 * tf.reduce_sum(kernel_cross * mask)\n","        ) / (batch_size * (batch_size - 1))\n","\n","        self.kid_record.update_state(kid)\n","\n","    def result(self):\n","        return self.kid_record.result()\n","\n","    def reset_state(self):\n","        self.kid_record.reset_state()\n","\n","class EMACallback(keras.callbacks.Callback):\n","    def __init__(self, ema_decay: float):\n","        super().__init__()\n","        self.ema_decay = ema_decay\n","\n","    def on_train_begin(self, logs=None):\n","        self.ema_weights = self.model.network.get_weights()\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        self.ema_weights = [\n","            self.ema_decay * ema_weight + (1 - self.ema_decay) * weight\n","            for ema_weight, weight in zip(\n","                self.ema_weights, self.model.network.get_weights()\n","            )\n","        ]\n","\n","    def on_test_begin(self, logs=None):\n","        self.backup = self.model.network.get_weights()\n","        self.model.network.set_weights(self.ema_weights)\n","\n","    def on_test_end(self, logs=None):\n","        self.model.network.set_weights(self.backup)\n","\n","    def on_train_end(self, logs=None):\n","        self.model.network.set_weights(self.ema_weights)\n","\n","\n","class SamplePlotCallback(keras.callbacks.Callback):\n","    def __init__(\n","        self,\n","        sample_embeddings,\n","        unconditional_sample_embeddings,\n","        diffusions_steps,\n","        num_rows,\n","        num_cols,\n","        plot_frequency,\n","        cfg_scale,\n","        save: bool = False,\n","        save_path: Path = None,\n","    ):\n","        super().__init__()\n","\n","        assert save_path is not None if save else True\n","\n","        self.sample_embeddings = sample_embeddings\n","        self.unconditional_sample_embeddings = unconditional_sample_embeddings\n","        self.diffusions_steps = diffusions_steps\n","        self.n_row = num_rows\n","        self.n_col = num_cols\n","        self.plot_freq = plot_frequency\n","        self.cfg_scale = cfg_scale\n","        self.save = save\n","        self.save_path = save_path\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if (epoch + 1) % self.plot_freq == 0:\n","            generate_images = self.model.plot_image(\n","                self.sample_embeddings,\n","                self.unconditional_sample_embeddings,\n","                self.n_row,\n","                self.n_col,\n","                self.diffusions_steps,\n","                self.cfg_scale,\n","            )\n","\n","        if self.save:\n","            plt.imsave(self.save_path, generate_images)\n","\n","\n","class PBarCallback(keras.callbacks.Callback):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def on_train_begin(self, logs=None):\n","        assert self.params[\"verbose\"] == 0, \"Set verbose=0 when using tqdm pbar\"\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        self.pbar = trange(\n","            self.params[\"steps\"],\n","            desc=f\"Epoch {epoch + 1}/{self.params['epochs']}\",\n","            colour=\"green\",\n","            unit=\"batch\",\n","        )\n","\n","    def on_batch_end(self, batch, logs=None):\n","        self.pbar.update(1)\n","        self.pbar.set_postfix({\n","            \"image_loss\": logs[\"image_loss\"],\n","            \"noise_loss\": logs[\"noise_loss\"],\n","            \"velocity_loss\": logs[\"velocity_loss\"],\n","        })\n","\n","    def on_test_begin(self, logs=None):\n","        self.pbar.colour = \"blue\"\n","        self.pbar.refresh()\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.pbar.colour = \"red\"\n","        self.pbar.set_postfix({\n","            \"val_image_loss\": logs[\"val_image_loss\"],\n","            \"val_noise_loss\": logs[\"val_noise_loss\"],\n","            \"val_velocity_loss\": logs[\"val_velocity_loss\"],\n","        })\n","        self.pbar.close()\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["ckpt_path = DirPath.checkpoint / \"diffusion.ckpt\"\n","ckpt_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=ckpt_path,\n","    save_weights_only=True,\n","    save_best_only=True,\n","    monitor=\"val_image_loss\",\n","    verbose=0,\n",")\n","ema_callback = EMACallback(TrainConfig.ema)\n","plot_callback = SamplePlotCallback(\n","    sample_embeddings,\n","    unconditional_sample_embeddings,\n","    TrainConfig.plot_diffusion_steps,\n","    num_rows=2,\n","    num_cols=5,\n","    plot_frequency=5,\n","    cfg_scale=TrainConfig.cfg_scale,\n",")\n","pbar_callback = PBarCallback()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["log_path = DirPath.log / f\"{timestamp}_loss.csv\"\n","params_path = DirPath.log / f\"{timestamp}_params.toml\"\n","csv_logger = keras.callbacks.CSVLogger(log_path, separator=\",\", append=False)\n","export_config(params_path)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Nomalizer adapting...\n"]}],"source":["print(\"Nomalizer adapting...\")\n","normalizer = tf.keras.layers.Normalization()\n","normalizer.adapt(dataset_train.map(lambda image, embedding: image))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def embedding_layer(embedding_max_freq, embedding_dim):\n","    def sinusoidal_embedding(x: tf.Tensor):\n","        min_freq = 1.0\n","        max_freq = embedding_max_freq\n","        freqs = tf.exp(\n","            tf.linspace(\n","                tf.math.log(min_freq),\n","                tf.math.log(max_freq),\n","                num=embedding_dim // 2,\n","            )\n","        )\n","        angular_speed = tf.cast(2 * math.pi * freqs, tf.float32)\n","\n","        embeddings = tf.concat(\n","            [tf.sin(x * angular_speed), tf.cos(x * angular_speed)], axis=-1\n","        )\n","\n","        return embeddings\n","\n","    return keras.layers.Lambda(sinusoidal_embedding)\n","\n","\n","def swish_glu(width):\n","    def apply(x):\n","        x = keras.layers.Dense(width * 2)(x)\n","        a, b = tf.split(x, 2, axis=-1)\n","        return a * keras.layers.Activation(\"swish\")(b)\n","\n","    return apply\n","\n","\n","def residual_block(width):\n","    def apply(input):\n","        x, n = input\n","        residual = keras.layers.Conv2D(width, 1)(x) if x.shape[-1] != width else x\n","\n","        x = keras.layers.GroupNormalization(epsilon=1e-5)(x)\n","        x = keras.layers.Activation(\"swish\")(x)\n","        x = keras.layers.Conv2D(width, 3, padding=\"same\")(x)\n","\n","        n = keras.layers.Activation(\"swish\")(n)\n","        n = keras.layers.Dense(width)(n)\n","\n","        x = keras.layers.Add()([x, n])\n","\n","        x = keras.layers.GroupNormalization(epsilon=1e-5)(x)\n","        x = keras.layers.Activation(\"swish\")(x)\n","        x = keras.layers.Conv2D(width, 3, padding=\"same\")(x)\n","\n","        x = keras.layers.Add()([x, residual])\n","\n","        return x\n","\n","    return apply\n","\n","\n","def basic_transformer_block(width, n_head=2):\n","    norm = keras.layers.GroupNormalization(epsilon=1e-5)\n","    attn = keras.layers.MultiHeadAttention(num_heads=n_head, key_dim=width // n_head)\n","\n","    def apply(input):\n","        x, t = input\n","        x = keras.layers.Dense(width, use_bias=False)(x)\n","        t = keras.layers.Dense(width, use_bias=False)(t)\n","\n","        x = attn(norm(x), norm(x)) + x\n","        x = attn(norm(x), t) + x\n","        return keras.layers.Dense(width)(swish_glu(width * 4)(norm(x))) + x\n","\n","    return apply\n","\n","\n","def spatial_transformer_block(width, n_head=2):\n","    def apply(input):\n","        x, t = input\n","        _, h, w, c = x.shape\n","        residual = x\n","\n","        x = keras.layers.GroupNormalization(epsilon=1e-5)(x)\n","        x = keras.layers.Conv2D(width, 1)(x)\n","        x = keras.layers.Reshape((h * w, c))(x)\n","        x = basic_transformer_block(width, n_head)([x, t])\n","        x = keras.layers.Reshape((h, w, c))(x)\n","\n","        return keras.layers.Conv2D(width, 1)(x) + residual\n","\n","    return apply\n","\n","\n","def downsampling_block(width):\n","    def apply(input):\n","        return keras.layers.Conv2D(width, 3, strides=2, padding=\"same\")(input)\n","\n","    return apply\n","\n","\n","def upsampling_block(width):\n","    def apply(input):\n","        x = keras.layers.UpSampling2D(2, interpolation=\"nearest\")(input)\n","        return keras.layers.Conv2D(width, 3, padding=\"same\")(x)\n","\n","    return apply\n","\n","\n","def get_network(\n","    image_size=64,\n","    image_embedding_dim=64,\n","    noise_embedding_dim=512,\n","    text_embedding_shape=(20, 512),\n","    widths=(64, 96, 128, 160),\n","    block_depth=2,\n","    embedding_max_frequency=1000.0,\n","):\n","    noisy_image = keras.layers.Input(shape=(image_size, image_size, 3))\n","    noise_power = keras.layers.Input(shape=(1, 1, 1))\n","    t_emb = keras.layers.Input(shape=text_embedding_shape)\n","\n","    n_emb = embedding_layer(embedding_max_frequency, noise_embedding_dim)(noise_power)\n","    n_emb = keras.layers.Dense(noise_embedding_dim, activation=\"swish\")(n_emb)\n","    n_emb = keras.layers.Dense(noise_embedding_dim)(n_emb)\n","\n","    x = keras.layers.Conv2D(image_embedding_dim, 1)(noisy_image)\n","\n","    skips = []\n","    x = residual_block(widths[0])([x, n_emb])\n","    x = residual_block(widths[0])([x, n_emb])\n","    x = downsampling_block(widths[0])(x)\n","    skips.append(x)\n","\n","    for width in widths[1:-1]:\n","        x = residual_block(width)([x, n_emb])\n","        x = spatial_transformer_block(width, n_head=2)([x, t_emb])\n","        skips.append(x)\n","\n","        x = residual_block(width)([x, n_emb])\n","        x = spatial_transformer_block(width, n_head=2)([x, t_emb])\n","        x = downsampling_block(width)(x)\n","        skips.append(x)\n","\n","    x = residual_block(widths[-1])([x, n_emb])\n","    x = residual_block(widths[-1])([x, n_emb])\n","    skips.append(x)\n","\n","    x = residual_block(widths[-1])([x, n_emb])\n","    x = spatial_transformer_block(widths[-1], n_head=2)([x, t_emb])\n","    x = residual_block(widths[-1])([x, n_emb])\n","\n","    x = keras.layers.Concatenate()([x, skips.pop()])\n","    x = residual_block(widths[-1])([x, n_emb])\n","    x = residual_block(widths[-1])([x, n_emb])\n","\n","    for width in reversed(widths[1:-1]):\n","        x = keras.layers.Concatenate()([x, skips.pop()])\n","        x = upsampling_block(width)(x)\n","        x = residual_block(width)([x, n_emb])\n","        x = spatial_transformer_block(width, n_head=2)([x, t_emb])\n","\n","        x = keras.layers.Concatenate()([x, skips.pop()])\n","        x = residual_block(width)([x, n_emb])\n","        x = spatial_transformer_block(width, n_head=2)([x, t_emb])\n","\n","    x = keras.layers.Concatenate()([x, skips.pop()])\n","    x = upsampling_block(widths[0])(x)\n","    x = residual_block(widths[0])([x, n_emb])\n","    x = residual_block(widths[0])([x, n_emb])\n","\n","    x = keras.layers.GroupNormalization(epsilon=1e-5)(x)\n","    x = keras.layers.Activation(\"swish\")(x)\n","    x = keras.layers.Conv2DTranspose(3, 1)(x)\n","\n","    return keras.Model(\n","        inputs=[noisy_image, noise_power, t_emb],\n","        outputs=x,\n","        name=\"noise_predictor\",\n","    )\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class DiffusionModel(keras.Model):\n","    def __init__(\n","        self,\n","        image_size,\n","        noise_embedding_dim,\n","        image_embedding_dim,\n","        text_embedding_shape,\n","        widths,\n","        block_depth,\n","        embedding_max_frequency,\n","        start_log_snr,\n","        end_log_snr,\n","    ):\n","        super().__init__()\n","\n","        self.network = get_network(\n","            image_size,\n","            noise_embedding_dim,\n","            image_embedding_dim,\n","            text_embedding_shape,\n","            widths,\n","            block_depth,\n","            embedding_max_frequency,\n","        )\n","\n","        self.image_size = image_size\n","        self.start_log_snr = start_log_snr\n","        self.end_log_snr = end_log_snr\n","\n","    def compile(self, normalizer, prediction_type: str = \"velocity\", **kwargs):\n","        super().compile(**kwargs)\n","        assert prediction_type in [\"noise\", \"image\", \"velocity\"]\n","\n","        self.prediction_type = prediction_type\n","        self.normalizer = normalizer\n","        self.noise_loss_tracker = keras.metrics.Mean(name=\"noise_loss\")\n","        self.image_loss_tracker = keras.metrics.Mean(name=\"image_loss\")\n","        self.velocity_loss_tracker = keras.metrics.Mean(name=\"velocity_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.noise_loss_tracker,\n","            self.image_loss_tracker,\n","            self.velocity_loss_tracker,\n","        ]\n","\n","    def denomalize(self, image):\n","        images = image * self.normalizer.variance**0.5 + self.normalizer.mean\n","        return tf.clip_by_value(images, 0.0, 1.0)\n","\n","    def diffusion_schedule(self, diffustion_times):\n","        start_snr = tf.math.exp(self.start_log_snr)\n","        end_snr = tf.math.exp(self.end_log_snr)\n","\n","        start_noise_power = 1.0 / (1.0 + start_snr)\n","        end_noise_power = 1.0 / (1.0 + end_snr)\n","\n","        noise_power = start_snr**diffustion_times / (\n","            start_snr * end_snr**diffustion_times + start_snr**diffustion_times\n","        )\n","        signal_power = 1.0 - noise_power\n","\n","        noise_rate = tf.math.sqrt(noise_power)\n","        signal_rate = tf.math.sqrt(signal_power)\n","\n","        return noise_rate, signal_rate\n","\n","    def get_component(self, noisy_images, predictions, signal_rates, noise_rates):\n","        if self.prediction_type == \"velocity\":\n","            pred_velocities = predictions\n","            pred_images = signal_rates * noisy_images - noise_rates * pred_velocities\n","            pred_noises = noise_rates * noisy_images + signal_rates * pred_velocities\n","\n","        elif self.prediction_type == \"image\":\n","            pred_images = predictions\n","            pred_noises = (noisy_images - signal_rates * pred_images) / noise_rates\n","            pred_velocities = (signal_rates * noisy_images - pred_images) / noise_rates\n","\n","        elif self.prediction_type == \"noise\":\n","            pred_noises = predictions\n","            pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n","            pred_velocities = (pred_noises - noise_rates * noisy_images) / signal_rates\n","\n","        return pred_noises, pred_images, pred_velocities\n","\n","    def denoise(self, noisy_images, text_embs, noise_rate, signal_rate, training):\n","        predictions = self.network(\n","            [noisy_images, noise_rate**2, text_embs], training=training\n","        )\n","        pred_noises, pred_images, pred_velocities = self.get_component(\n","            noisy_images, predictions, signal_rate, noise_rate\n","        )\n","        return pred_noises, pred_images, pred_velocities\n","\n","    def reverse_diffusion(\n","        self, initial_noise, text_embs, un_text_embs, diffusion_steps, cfg_scale\n","    ):\n","        batch_size = tf.shape(initial_noise)[0]\n","        step_size = 1.0 / diffusion_steps\n","\n","        noisy_images = initial_noise\n","        for step in range(diffusion_steps):\n","            diffusion_times = tf.ones([batch_size, 1, 1, 1]) - step_size * step\n","            noise_rate, signal_rate = self.diffusion_schedule(diffusion_times)\n","\n","            pred_noises, pred_images, _ = self.denoise(\n","                noisy_images, text_embs, noise_rate, signal_rate, training=False\n","            )\n","            un_pred_noises, _, _ = self.denoise(\n","                noisy_images, un_text_embs, noise_rate, signal_rate, training=False\n","            )\n","\n","            pred_noises = un_pred_noises + cfg_scale * (pred_noises - un_pred_noises)\n","            pred_images = (noisy_images - noise_rate * pred_noises) / signal_rate\n","\n","            next_diffusion_times = diffusion_times - step_size\n","            next_noise_rate, next_signal_rate = self.diffusion_schedule(\n","                next_diffusion_times\n","            )\n","            noisy_images = (\n","                next_signal_rate * pred_images + next_noise_rate * pred_noises\n","            )\n","\n","        return pred_images\n","\n","    def generate(self, num_images, text_embs, un_text_embs, diffusion_steps, cfg_scale):\n","        initial_noise = tf.random.normal(\n","            (num_images, self.image_size, self.image_size, 3), seed=RANDOM_STATE\n","        )\n","        generated_images = self.reverse_diffusion(\n","            initial_noise, text_embs, un_text_embs, diffusion_steps, cfg_scale\n","        )\n","        generated_images = self.denomalize(generated_images)\n","        return generated_images\n","\n","    def train_step(self, input):\n","        images, text_embs = input\n","\n","        images = self.normalizer(images, training=True)\n","        noises = tf.random.normal(tf.shape(images), seed=RANDOM_STATE)\n","\n","        noise_powers = tf.random.uniform(\n","            [images.shape[0], 1, 1, 1], 0.0, 1.0, seed=RANDOM_STATE\n","        )\n","        signal_powers = 1.0 - noise_powers\n","        noise_rates = tf.math.sqrt(noise_powers)\n","        signal_rates = tf.math.sqrt(signal_powers)\n","\n","        noisy_images = signal_rates * images + noise_rates * noises\n","        velocities = -noise_rates * images + signal_rates * noises\n","\n","        with tf.GradientTape() as tape:\n","            pred_noises, pred_images, pred_velocity = self.denoise(\n","                noisy_images, text_embs, noise_rates, signal_rates, training=True\n","            )\n","            velocity_loss = self.loss(velocities, pred_velocity)\n","            image_loss = self.loss(images, pred_images)\n","            noise_loss = self.loss(noises, pred_noises)\n","\n","        if self.prediction_type == \"noise\":\n","            loss = noise_loss\n","        elif self.prediction_type == \"image\":\n","            loss = image_loss\n","        elif self.prediction_type == \"velocity\":\n","            loss = velocity_loss\n","\n","        gradients = tape.gradient(loss, self.network.trainable_weights)\n","        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n","\n","        self.velocity_loss_tracker.update_state(velocity_loss)\n","        self.image_loss_tracker.update_state(image_loss)\n","        self.noise_loss_tracker.update_state(noise_loss)\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def test_step(self, input):\n","        images, text_embs = input\n","\n","        images = self.normalizer(images, training=False)\n","        noises = tf.random.normal(tf.shape(images), seed=RANDOM_STATE)\n","\n","        noise_powers = tf.random.uniform(\n","            [images.shape[0], 1, 1, 1], 0.0, 1.0, seed=RANDOM_STATE\n","        )\n","        signal_powers = 1.0 - noise_powers\n","        noise_rates = tf.math.sqrt(noise_powers)\n","        signal_rates = tf.math.sqrt(signal_powers)\n","\n","        noisy_images = signal_rates * images + noise_rates * noises\n","        velocities = -noise_rates * images + signal_rates * noises\n","\n","        pred_noises, pred_images, pred_velocity = self.denoise(\n","            noisy_images, text_embs, noise_rates, signal_rates, training=False\n","        )\n","        velocity_loss = self.loss(velocities, pred_velocity)\n","        image_loss = self.loss(images, pred_images)\n","        noise_loss = self.loss(noises, pred_noises)\n","\n","        self.velocity_loss_tracker.update_state(velocity_loss)\n","        self.image_loss_tracker.update_state(image_loss)\n","        self.noise_loss_tracker.update_state(noise_loss)\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def plot_image(\n","        self,\n","        text_embeddings,\n","        un_text_embs,\n","        num_rows,\n","        num_cols,\n","        diffusion_steps,\n","        cfg_scale,\n","    ):\n","        generate_images = self.generate(\n","            num_rows * num_cols,\n","            text_embeddings,\n","            un_text_embs,\n","            diffusion_steps=diffusion_steps,\n","            cfg_scale=cfg_scale,\n","        )\n","        generate_images = tf.reshape(\n","            generate_images, (num_rows, num_cols, self.image_size, self.image_size, 3)\n","        )\n","        generate_images = tf.transpose(generate_images, (0, 2, 1, 3, 4))\n","        generate_images = tf.reshape(\n","            generate_images,\n","            (num_rows * self.image_size, num_cols * self.image_size, 3),\n","        )\n","\n","        plt.figure(figsize=(num_cols * 1.5, num_rows * 1.5))\n","        plt.imshow(generate_images.numpy())\n","        plt.axis(\"off\")\n","        plt.tight_layout()\n","        plt.show()\n","        plt.close()\n","\n","        return generate_images.numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = DiffusionModel(**asdict(ModelConfig()))\n","model.compile(\n","    prediction_type=\"velocity\",\n","    normalizer=normalizer,\n","    optimizer=keras.optimizers.Lion(\n","        learning_rate=TrainConfig.lr_init,\n","        weight_decay=TrainConfig.weight_decay,\n","    ),\n","    loss=keras.losses.mean_absolute_error,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if TrainConfig.transfer:\n","    model.load_weights(ckpt_path)\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50:   0%|\u001b[32m          \u001b[0m| 1/1174 [00:15<5:07:01, 15.70s/batch]\n"]},{"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3 defined at (most recent call last):\n<stack traces unavailable>\nDetected at node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:   OOM when allocating tensor with shape[1,500,698,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_4]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:   OOM when allocating tensor with shape[1,500,698,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_151458]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[1;32m/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_窩不知道誒_report.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     dataset_train,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mdataset_val,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mTrainConfig\u001b[39m.\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         ckpt_callback,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         csv_logger,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         pbar_callback,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         ema_callback,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         plot_callback,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/DL_comp3_%E7%AA%A9%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AA%92_report.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3 defined at (most recent call last):\n<stack traces unavailable>\nDetected at node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:   OOM when allocating tensor with shape[1,500,698,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_4]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:   OOM when allocating tensor with shape[1,500,698,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node cond/sequential/random_rotation/transform/ImageProjectiveTransformV3}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_151458]"]}],"source":["print(\"Start training...\")\n","model.fit(\n","    dataset_train,\n","    validation_data=dataset_val,\n","    epochs=TrainConfig.epochs,\n","    verbose=0,\n","    callbacks=[\n","        ckpt_callback,\n","        csv_logger,\n","        pbar_callback,\n","        ema_callback,\n","        plot_callback,\n","    ],\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loss curve"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log = pd.read_csv(log_path)\n","train_image_loss = log[\"image_loss\"]\n","train_noise_loss = log[\"noise_loss\"]\n","val_image_loss = log[\"val_image_loss\"]\n","val_noise_loss = log[\"val_noise_loss\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(train_image_loss, label=\"train_image_loss\", color=\"blue\")\n","plt.plot(val_image_loss, label=\"val_image_loss\", linestyle=\"--\", color=\"blue\")\n","plt.plot(train_noise_loss, label=\"train_noise_loss\", color=\"orange\")\n","plt.plot(val_noise_loss, label=\"val_noise_loss\", linestyle=\"--\", color=\"orange\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Load best model...\")\n","model.load_weights(ckpt_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if (DirPath.output / \"inference\").exists():\n","    shutil.rmtree(DirPath.output / \"inference\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(DirPath.output / \"inference\").mkdir(parents=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_epoch = len(df_test) // TrainConfig.batch_size + 1\n","step = 0\n","for id, text_embeddings in tqdm(\n","    dataset_test, total=test_epoch, desc=\"Generate image:\", colour=\"green\"\n","):\n","    step += 1\n","    if step > test_epoch:\n","        break\n","    generated_images = model.generate(\n","        num_images=TrainConfig.batch_size,\n","        text_embs=text_embeddings,\n","        un_text_embs=unconditional_test_embeddings,\n","        diffusion_steps=TrainConfig.plot_diffusion_steps,\n","        cfg_scale=TrainConfig.cfg_scale,\n","    )\n","    for i, img in enumerate(generated_images):\n","        plt.imsave(\n","            DirPath.output / f\"inference/inference_{id[i]:04d}.jpg\",\n","            img.numpy(),\n","            vmin=0.0,\n","            vmax=1.0,\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.chdir(\"./evaluation\")\n","os.system(\"python inception_score.py ../output/inference ../output/score.csv 39\")\n","os.chdir(\"..\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_score = pd.read_csv(\"./output/score.csv\")\n","print(f\"Score: {np.mean(df_score['score']):.4f} ± {np.std(df_score['score']):.4f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
