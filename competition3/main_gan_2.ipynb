{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m gpus \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mlist_physical_devices(\u001b[39m'\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/__init__.py:41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_six\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     44\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/__init__.py:40\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     43\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m attr_value_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m node_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_def_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_handle_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2 \u001b[39mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[39m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[39m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[39m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36\u001b[0m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m   serialized_pb\u001b[39m=\u001b[39m_b(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mtensorflow\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mz\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39munknown_rank\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x44\u001b[39;00m\u001b[39mim\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mname\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mB\u001b[39m\u001b[39m\\x87\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x18\u001b[39;00m\u001b[39morg.tensorflow.frameworkB\u001b[39m\u001b[39m\\x11\u001b[39;00m\u001b[39mTensorShapeProtosP\u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[39m\\xf8\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     30\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m   filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m   file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     34\u001b[0m   containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m   fields\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 36\u001b[0m     _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[1;32m     37\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m, full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m       number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, cpp_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m       has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, default_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m       message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     41\u001b[0m       is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     42\u001b[0m       serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, cpp_type\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m       has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, default_value\u001b[39m=\u001b[39m_b(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m       message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m       is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   extensions\u001b[39m=\u001b[39m[\n\u001b[1;32m     52\u001b[0m   ],\n\u001b[1;32m     53\u001b[0m   nested_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     54\u001b[0m   enum_types\u001b[39m=\u001b[39m[\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m   is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m   syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m   extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[1;32m     60\u001b[0m   oneofs\u001b[39m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m   ],\n\u001b[1;32m     62\u001b[0m   serialized_start\u001b[39m=\u001b[39m\u001b[39m149\u001b[39m,\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow.TensorShapeProto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[39m.\u001b[39mcontaining_type \u001b[39m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39mdefault_pool\u001b[39m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "\n",
    "def make_puzzle(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h: (j + 1) * h, i * w: (i + 1) * w, :] = img\n",
    "    if path is not None:\n",
    "        imageio.imwrite(path, out)\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "SAMPLE_ROW = 8\n",
    "SAMPLE_COL = 8\n",
    "SAMPLE_NUM = SAMPLE_ROW * SAMPLE_COL\n",
    "\n",
    "hparams = {\n",
    "    'HIDDEN_DIM': 64,\n",
    "    'DENSE_DIM': 128,\n",
    "    'Z_DIM': 100,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BETA_1': 0.5,\n",
    "    'BETA_2': 0.999,\n",
    "    'LAMBDA': 10,\n",
    "    'NUM_EPOCH': 100,\n",
    "    'NOISE_DECAY_LIMIT': 30,\n",
    "    'CHECKPOINTS_DIR': './checkpoints/ckpt/',\n",
    "    'TRAIN_FROM_LATEST_EPOCH': False,\n",
    "    'PRINT_FREQ': 10\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dir = './data/dictionary/'\n",
    "word2Id_dict = dict(np.load(dictionary_dir + 'word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_dir + 'id2Word.npy'))\n",
    "\n",
    "\n",
    "def indices_list_to_text_list(indices_list):\n",
    "    text_list = []\n",
    "    for indices in indices_list:\n",
    "        word_list = []\n",
    "        for idx in indices:\n",
    "            if idx == word2Id_dict['<RARE>']:\n",
    "                continue\n",
    "            if idx == word2Id_dict['<PAD>']:\n",
    "                break\n",
    "            word_list.append(id2word_dict[idx])\n",
    "        text = ' '.join(word_list)\n",
    "        if len(text.strip()) != 0:\n",
    "            text_list.append(text)\n",
    "    return text_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding with Sentence-BERT\n",
    "\n",
    "Install `SentenceTransformer`\n",
    "```sh\n",
    "$ conda install -c conda-forge sentence-transformers\n",
    "$ conda install -c conda-forge ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir = './data/dataset/'\n",
    "if os.path.exists(dataset_dir + 'train_data_embedding.pkl'):\n",
    "    print(\"if\")\n",
    "    df_train = pd.read_pickle(dataset_dir + 'train_data_embedding.pkl')\n",
    "    df_train.head()\n",
    "else:\n",
    "    print(\"el\")\n",
    "    sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "    df_train = pd.read_pickle(dataset_dir + 'text2ImgData.pkl')\n",
    "    df_train['Texts'] = df_train['Captions'].apply(lambda x: indices_list_to_text_list(x))\n",
    "    df_train['Embeddings'] = df_train['Texts'].apply(lambda x: sbert.encode(x))\n",
    "    df_train.head()\n",
    "    df_train.to_pickle(dataset_dir + 'train_data_embedding.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ------------\n",
      "absl-py                       2.0.0\n",
      "albumentations                1.3.1\n",
      "asttokens                     2.4.1\n",
      "astunparse                    1.6.3\n",
      "backports.functools-lru-cache 1.6.5\n",
      "cachetools                    5.3.2\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.16.0\n",
      "charset-normalizer            3.3.2\n",
      "clang                         5.0\n",
      "click                         8.1.7\n",
      "colorama                      0.4.6\n",
      "comm                          0.1.4\n",
      "cryptography                  41.0.5\n",
      "cycler                        0.12.1\n",
      "debugpy                       1.6.7\n",
      "decorator                     4.4.2\n",
      "defusedxml                    0.7.1\n",
      "entrypoints                   0.4\n",
      "exceptiongroup                1.1.3\n",
      "executing                     2.0.1\n",
      "filelock                      3.13.1\n",
      "flatbuffers                   23.5.26\n",
      "fsspec                        2023.12.2\n",
      "gast                          0.4.0\n",
      "Glances                       3.4.0.3\n",
      "google-auth                   2.23.4\n",
      "google-auth-oauthlib          1.1.0\n",
      "google-pasta                  0.2.0\n",
      "grpcio                        1.59.2\n",
      "h5py                          3.1.0\n",
      "huggingface-hub               0.19.4\n",
      "icecream                      2.1.3\n",
      "idna                          3.4\n",
      "imageio                       2.33.0\n",
      "imageio-ffmpeg                0.4.9\n",
      "importlib-metadata            6.8.0\n",
      "ipykernel                     6.26.0\n",
      "ipython                       8.17.2\n",
      "jedi                          0.19.1\n",
      "Jinja2                        3.1.2\n",
      "joblib                        1.3.2\n",
      "jupyter-client                7.3.4\n",
      "jupyter_core                  5.5.0\n",
      "keras                         2.15.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "kiwisolver                    1.4.5\n",
      "lazy_loader                   0.3\n",
      "libclang                      16.0.6\n",
      "Markdown                      3.5.1\n",
      "MarkupSafe                    2.1.3\n",
      "matplotlib                    3.3.4\n",
      "matplotlib-inline             0.1.6\n",
      "ml-dtypes                     0.2.0\n",
      "moviepy                       1.0.3\n",
      "mpmath                        1.3.0\n",
      "nest-asyncio                  1.5.8\n",
      "networkx                      3.2.1\n",
      "nltk                          3.8.1\n",
      "numpy                         1.24.4\n",
      "nvidia-cublas-cu12            12.1.3.1\n",
      "nvidia-cuda-cupti-cu12        12.1.105\n",
      "nvidia-cuda-nvrtc-cu12        12.1.105\n",
      "nvidia-cuda-runtime-cu12      12.1.105\n",
      "nvidia-cudnn-cu12             8.9.2.26\n",
      "nvidia-cufft-cu12             11.0.2.54\n",
      "nvidia-curand-cu12            10.3.2.106\n",
      "nvidia-cusolver-cu12          11.4.5.107\n",
      "nvidia-cusparse-cu12          12.1.0.106\n",
      "nvidia-nccl-cu12              2.18.1\n",
      "nvidia-nvjitlink-cu12         12.3.101\n",
      "nvidia-nvtx-cu12              12.1.105\n",
      "oauthlib                      3.2.2\n",
      "opencv-python                 4.8.1.78\n",
      "opencv-python-headless        4.8.1.78\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     23.2\n",
      "pandas                        1.4.3\n",
      "parso                         0.8.3\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        10.1.0\n",
      "pip                           23.3\n",
      "platformdirs                  3.11.0\n",
      "proglog                       0.1.10\n",
      "prompt-toolkit                3.0.39\n",
      "protobuf                      4.23.4\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "pyasn1                        0.5.0\n",
      "pyasn1-modules                0.3.0\n",
      "pycparser                     2.21\n",
      "Pygments                      2.16.1\n",
      "pyparsing                     3.1.1\n",
      "python-dateutil               2.8.2\n",
      "pytz                          2023.3.post1\n",
      "PyYAML                        6.0.1\n",
      "pyzmq                         25.1.0\n",
      "qudida                        0.0.4\n",
      "regex                         2023.10.3\n",
      "requests                      2.31.0\n",
      "requests-oauthlib             1.3.1\n",
      "rsa                           4.9\n",
      "safetensors                   0.4.1\n",
      "scikit-image                  0.22.0\n",
      "scikit-learn                  1.3.2\n",
      "scipy                         1.8.0\n",
      "sentence-transformers         2.2.2\n",
      "sentencepiece                 0.1.99\n",
      "setuptools                    68.0.0\n",
      "six                           1.15.0\n",
      "stack-data                    0.6.2\n",
      "sympy                         1.12\n",
      "tensorboard                   2.15.1\n",
      "tensorboard-data-server       0.7.2\n",
      "tensorflow                    2.15.0.post1\n",
      "tensorflow-estimator          2.15.0\n",
      "tensorflow-gpu                2.6.0\n",
      "tensorflow-io-gcs-filesystem  0.34.0\n",
      "termcolor                     1.1.0\n",
      "threadpoolctl                 3.2.0\n",
      "tifffile                      2023.12.9\n",
      "tokenizers                    0.15.0\n",
      "toml                          0.10.2\n",
      "torch                         2.1.2\n",
      "torchvision                   0.16.2\n",
      "tornado                       6.1\n",
      "tqdm                          4.66.1\n",
      "traitlets                     5.13.0\n",
      "transformers                  4.36.1\n",
      "triton                        2.1.0\n",
      "typing-extensions             3.7.4.3\n",
      "tzdata                        2023.3\n",
      "ujson                         5.8.0\n",
      "urllib3                       2.0.7\n",
      "wcwidth                       0.2.9\n",
      "Werkzeug                      3.0.1\n",
      "wheel                         0.41.2\n",
      "wrapt                         1.12.1\n",
      "zipp                          3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Texts</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>[[0.049325254, -0.08779175, -0.04624424, 0.074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>[[-0.022878254, -0.02727319, -0.019405542, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>[[0.004260789, -0.02295248, -0.04379613, 0.040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>[[0.035809662, -0.020764334, -0.03761639, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>[[-0.014632274, -0.072241016, -0.046013944, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                                  Texts  \\\n",
       "ID                                                        \n",
       "6734  [the petals of the flower are pink in color an...   \n",
       "6736  [this flower has white petals and yellow pisti...   \n",
       "6737  [the petals on this flower are pink with white...   \n",
       "6738  [the flower has a smooth purple petal with whi...   \n",
       "6739  [this white flower has bright yellow stamen wi...   \n",
       "\n",
       "                                             Embeddings  \n",
       "ID                                                       \n",
       "6734  [[0.049325254, -0.08779175, -0.04624424, 0.074...  \n",
       "6736  [[-0.022878254, -0.02727319, -0.019405542, 0.0...  \n",
       "6737  [[0.004260789, -0.02295248, -0.04379613, 0.040...  \n",
       "6738  [[0.035809662, -0.020764334, -0.03761639, 0.06...  \n",
       "6739  [[-0.014632274, -0.072241016, -0.046013944, 0....  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def map_train(embedding, image_path):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=IMAGE_CHANNEL)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, IMAGE_CHANNEL])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = img * 2 - 1\n",
    "\n",
    "    return embedding, img\n",
    "\n",
    "\n",
    "def train_dataset_generator(embedding_array, image_path_array, batch_size):\n",
    "    embedding_list = []\n",
    "    image_path_list = []\n",
    "    for embeddings, image_path in zip(embedding_array, image_path_array):\n",
    "        for embedding in embeddings:\n",
    "            embedding_list.append(embedding)\n",
    "            image_path_list.append(image_path)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding_list, image_path_list))\n",
    "    dataset = dataset.map(map_train, tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(embedding_list))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70400 samples in training data.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = train_dataset_generator(df_train['Embeddings'].values, df_train['ImagePath'].values, hparams['BATCH_SIZE'])\n",
    "print(f'There are {len(dataset_train) * hparams[\"BATCH_SIZE\"]:d} samples in training data.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GAN Model (DCGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "batchnorm_init = tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_batchnorm_relu(filters):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=filters,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='SAME',\n",
    "            output_padding=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=conv_init\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(gamma_initializer=batchnorm_init),\n",
    "        tf.keras.layers.ReLU()\n",
    "    ])\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hparams):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.dim = self.hparams['HIDDEN_DIM']\n",
    "\n",
    "        self.embedding_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.hparams['DENSE_DIM']),\n",
    "            tf.keras.layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "\n",
    "        self.concat_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.dim * 8 * 4 * 4, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(gamma_initializer=batchnorm_init),\n",
    "            tf.keras.layers.ReLU(),\n",
    "\n",
    "            tf.keras.layers.Reshape([4, 4, -1]),\n",
    "\n",
    "            deconv_batchnorm_relu(self.dim * 4),\n",
    "            deconv_batchnorm_relu(self.dim * 2),\n",
    "            deconv_batchnorm_relu(self.dim * 1),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=IMAGE_CHANNEL,\n",
    "                kernel_size=5,\n",
    "                strides=2,\n",
    "                padding='SAME',\n",
    "                output_padding=1,\n",
    "                activation=tf.keras.activations.tanh,\n",
    "                kernel_initializer=conv_init\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def call(self, embedding, noise_z):\n",
    "        embedding = self.embedding_layers(embedding)\n",
    "\n",
    "        concat = tf.concat([noise_z, embedding], axis=1)\n",
    "\n",
    "        output = self.concat_layers(concat)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_batchnorm_leaky_relu(filters):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='SAME',\n",
    "            kernel_initializer=conv_init\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(gamma_initializer=batchnorm_init),\n",
    "        tf.keras.layers.LeakyReLU(0.2)\n",
    "    ])\n",
    "\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, hparams):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.dim = self.hparams['HIDDEN_DIM']\n",
    "\n",
    "        self.embedding_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.hparams['DENSE_DIM']),\n",
    "            tf.keras.layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "\n",
    "        self.image_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=self.dim,\n",
    "                kernel_size=5,\n",
    "                strides=2,\n",
    "                padding='SAME',\n",
    "                kernel_initializer=conv_init\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(0.2),\n",
    "            conv_batchnorm_leaky_relu(self.dim * 2),\n",
    "            conv_batchnorm_leaky_relu(self.dim * 4),\n",
    "            conv_batchnorm_leaky_relu(self.dim * 8)\n",
    "        ])\n",
    "\n",
    "        self.concat_layers = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=self.dim * 8,\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                padding='SAME',\n",
    "                kernel_initializer=conv_init\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(0.2),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=1,\n",
    "                kernel_size=4,\n",
    "                strides=1,\n",
    "                padding='VALID',\n",
    "                kernel_initializer=conv_init\n",
    "            ),\n",
    "            tf.keras.layers.Reshape([])\n",
    "        ])\n",
    "\n",
    "    def call(self, embedding, image):\n",
    "        embedding = self.embedding_layers(embedding)\n",
    "        embedding = tf.expand_dims(embedding, axis=1)\n",
    "        embedding = tf.expand_dims(embedding, axis=1)\n",
    "        embedding = tf.tile(embedding, multiples=[1, 4, 4, 1])\n",
    "\n",
    "        image = self.image_layers(image)\n",
    "\n",
    "        concat = tf.concat([image, embedding], axis=-1)\n",
    "\n",
    "        output = self.concat_layers(concat)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hparams)\n",
    "discriminator = Discriminator(hparams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(hparams['LEARNING_RATE'], hparams['BETA_1'], hparams['BETA_2'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparams['LEARNING_RATE'], hparams['BETA_1'], hparams['BETA_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def G_train_step(embedding, real_data, noise_decay):\n",
    "    noise_z = tf.random.normal([hparams['BATCH_SIZE'], hparams['Z_DIM']])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_data = generator(embedding, noise_z, training=True)\n",
    "        fake_data = fake_data + noise_decay * tf.random.normal(fake_data.shape)\n",
    "        real_data = real_data + noise_decay * tf.random.normal(real_data.shape)\n",
    "\n",
    "        D_fake = discriminator(embedding, fake_data, training=True)\n",
    "        D_real = discriminator(embedding, real_data, training=True)\n",
    "\n",
    "        loss_g = tf.reduce_mean(-D_fake)\n",
    "\n",
    "        epsilon = tf.random.uniform([hparams['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        slopes = tf.gradients(discriminator(embedding, interpolates, training=True), interpolates)[0]\n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(slopes ** 2, axis=[1, 2, 3]))\n",
    "        gradient_penalty = (l2_norm - 1.) ** 2\n",
    "\n",
    "        loss_d = tf.reduce_mean(D_fake - D_real + hparams['LAMBDA'] * gradient_penalty)\n",
    "\n",
    "    gradient = tape.gradient(loss_g, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradient, generator.trainable_variables))\n",
    "\n",
    "    return loss_g, loss_d\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def D_train_step(embedding, real_data, noise_decay):\n",
    "    noise_z = tf.random.normal([hparams['BATCH_SIZE'], hparams['Z_DIM']])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_data = generator(embedding, noise_z, training=True)\n",
    "        fake_data = fake_data + noise_decay * tf.random.normal(fake_data.shape)\n",
    "        real_data = real_data + noise_decay * tf.random.normal(real_data.shape)\n",
    "\n",
    "        D_fake = discriminator(embedding, fake_data, training=True)\n",
    "        D_real = discriminator(embedding, real_data, training=True)\n",
    "\n",
    "        loss_g = tf.reduce_mean(-D_fake)\n",
    "\n",
    "        epsilon = tf.random.uniform([hparams['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        slopes = tf.gradients(discriminator(embedding, interpolates, training=True), interpolates)[0]\n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(slopes ** 2, axis=[1, 2, 3]))\n",
    "        gradient_penalty = (l2_norm - 1.) ** 2\n",
    "\n",
    "        loss_d = tf.reduce_mean(D_fake - D_real + hparams['LAMBDA'] * gradient_penalty)\n",
    "\n",
    "    gradient = tape.gradient(loss_d, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient, discriminator.trainable_variables))\n",
    "\n",
    "    return loss_g, loss_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = (\n",
    "    D_train_step,\n",
    "    D_train_step,\n",
    "    G_train_step\n",
    ")\n",
    "\n",
    "num_critic = len(train_step)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Training Sample for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dir = './samples/'\n",
    "if not os.path.exists(samples_dir):\n",
    "    os.makedirs(samples_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 768)\n"
     ]
    }
   ],
   "source": [
    "sample_embeddings = []\n",
    "for embeddings in df_train['Embeddings'].values:\n",
    "    if len(embeddings) >= SAMPLE_ROW:\n",
    "        for j in range(SAMPLE_ROW):\n",
    "            sample_embeddings.append(embeddings[j])\n",
    "    if len(sample_embeddings) == SAMPLE_NUM:\n",
    "        break\n",
    "\n",
    "sample_embeddings = tf.Variable(sample_embeddings)\n",
    "print(sample_embeddings.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, hparams['CHECKPOINTS_DIR'], max_to_keep=20)\n",
    "\n",
    "start_epoch = 0\n",
    "if hparams['TRAIN_FROM_LATEST_EPOCH'] and checkpoint_manager.latest_checkpoint:\n",
    "    start_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(start_epoch, stop_epoch):\n",
    "    g_loss_list = [None] * (stop_epoch - start_epoch)\n",
    "    d_loss_list = [None] * (stop_epoch - start_epoch)\n",
    "\n",
    "    critic = 0\n",
    "    num_train_step = len(dataset_train)\n",
    "    sample_noise = tf.random.normal([SAMPLE_NUM, hparams['Z_DIM']])\n",
    "\n",
    "    pbar = trange(start_epoch, stop_epoch, unit='epoch')\n",
    "    for epoch in pbar:\n",
    "        loss_g_t = 0.0\n",
    "        loss_d_t = 0.0\n",
    "\n",
    "        if epoch < hparams['NOISE_DECAY_LIMIT']:\n",
    "            noise_decay = 1 / (epoch + 1)\n",
    "        else:\n",
    "            noise_decay = 0\n",
    "\n",
    "        for embedding, real_data in dataset_train:\n",
    "            loss_g, loss_d = train_step[critic](embedding, real_data, noise_decay)\n",
    "            critic = critic + 1 if critic + 1 < num_critic else 0\n",
    "            loss_g_t += loss_g.numpy()\n",
    "            loss_d_t += loss_d.numpy()\n",
    "\n",
    "        g_loss_list[epoch] = loss_g_t / num_train_step\n",
    "        d_loss_list[epoch] = loss_d_t / num_train_step\n",
    "        pbar.set_postfix({'g_loss': loss_g_t / num_train_step, 'd_loss': loss_d_t / num_train_step})\n",
    "\n",
    "        out = generator(sample_embeddings, sample_noise, training=False)\n",
    "        img = make_puzzle(\n",
    "            ((out + 1) / 2 * 255).numpy().astype(np.uint8),\n",
    "            SAMPLE_ROW,\n",
    "            SAMPLE_COL,\n",
    "            f'{samples_dir}train_{epoch + 1:04d}.png'\n",
    "        )\n",
    "        if (epoch + 1) % hparams['PRINT_FREQ'] == 0:\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Epoch {epoch + 1:d}')\n",
    "            plt.show()\n",
    "\n",
    "        checkpoint_manager.save(epoch + 1)\n",
    "\n",
    "    return g_loss_list, d_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.6\n",
      "  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting numpy~=1.19.2 (from tensorflow==2.6)\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py~=0.10 (from tensorflow==2.6)\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.6.3)\n",
      "Requirement already satisfied: clang~=5.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (5.0)\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow==2.6)\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (4.23.4)\n",
      "Requirement already satisfied: six~=1.15.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (0.41.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (2.15.0)\n",
      "Requirement already satisfied: keras~=2.6 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.6) (1.59.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard~=2.6->tensorflow==2.6) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6) (3.2.2)\n",
      "Installing collected packages: flatbuffers, numpy, absl-py, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages/~.mpy.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages/~.mpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.0\n",
      "    Uninstalling tensorflow-2.8.0:\n",
      "      Successfully uninstalled tensorflow-2.8.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/sclab-nas_homes/@LH-SCLAB.CS.NTHU.EDU.TW/61/wesley-1000028/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/~latform'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ml-dtypes 0.2.0 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "scikit-image 0.22.0 requires numpy>=1.22, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 numpy-1.19.5 tensorflow-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?epoch/s]2023-12-16 23:25:40.113587: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 11187 of 70495\n",
      "2023-12-16 23:25:50.112374: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 22540 of 70495\n",
      "2023-12-16 23:26:00.114399: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 34559 of 70495\n",
      "2023-12-16 23:26:13.877666: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 46469 of 70495\n",
      "2023-12-16 23:26:30.122094: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 67415 of 70495\n",
      "2023-12-16 23:26:33.052305: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n",
      "/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-12-16 23:26:35.327963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-12-16 23:26:35.328860: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_ops_impl.h:1199 : UNIMPLEMENTED: DNN library is not found.\n",
      "2023-12-16 23:26:35.690571: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-12-16 23:26:35.691390: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_grad_input_ops.cc:386 : UNIMPLEMENTED: DNN library is not found.\n",
      "  0%|          | 0/100 [01:05<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node discriminator/sequential_9/conv2d/Conv2D_1 defined at (most recent call last):\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 737, in start\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n\n  File \"/tmp/ipykernel_18010/1863706989.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_18010/3863691208.py\", line 25, in train\n\n  File \"/tmp/ipykernel_18010/2513557891.py\", line 40, in D_train_step\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/tmp/ipykernel_18010/3780318260.py\", line 65, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py\", line 290, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py\", line 262, in convolution_op\n\nDNN library is not found.\n\t [[{{node discriminator/sequential_9/conv2d/Conv2D_1}}]] [Op:__inference_D_train_step_5399]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m g_loss_list, d_loss_list \u001b[39m=\u001b[39m train(start_epoch, hparams[\u001b[39m'\u001b[39;49m\u001b[39mNUM_EPOCH\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32m/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     noise_decay \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m embedding, real_data \u001b[39min\u001b[39;00m dataset_train:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     loss_g, loss_d \u001b[39m=\u001b[39m train_step[critic](embedding, real_data, noise_decay)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     critic \u001b[39m=\u001b[39m critic \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m critic \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m num_critic \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.75.232/home/wesley/sclab-nas_home/courses/m1_1/dl/1121_DL_Competition/competition3/main_gan_2.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     loss_g_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_g\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node discriminator/sequential_9/conv2d/Conv2D_1 defined at (most recent call last):\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 737, in start\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n\n  File \"/tmp/ipykernel_18010/1863706989.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_18010/3863691208.py\", line 25, in train\n\n  File \"/tmp/ipykernel_18010/2513557891.py\", line 40, in D_train_step\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/tmp/ipykernel_18010/3780318260.py\", line 65, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py\", line 290, in call\n\n  File \"/home/wesley/sclab-nas_home/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py\", line 262, in convolution_op\n\nDNN library is not found.\n\t [[{{node discriminator/sequential_9/conv2d/Conv2D_1}}]] [Op:__inference_D_train_step_5399]"
     ]
    }
   ],
   "source": [
    "g_loss_list, d_loss_list = train(start_epoch, hparams['NUM_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(start_epoch, hparams['NUM_EPOCH']), g_loss_list, color='red', label='Generator Loss')\n",
    "plt.plot(range(start_epoch, hparams['NUM_EPOCH']), d_loss_list, color='blue', label='Discriminator Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('WGAN-GP Training Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(dataset_dir + 'test_data_embedding.pkl'):\n",
    "    df_test = pd.read_pickle(dataset_dir + 'test_data_embedding.pkl')\n",
    "else:\n",
    "    sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "    df_test = pd.read_pickle(dataset_dir + 'testData.pkl')\n",
    "    df_test['Texts'] = df_test['Captions'].apply(lambda x: indices_list_to_text_list([x]))\n",
    "    df_test['Embeddings'] = df_test['Texts'].apply(lambda x: sbert.encode(x)[0])\n",
    "    df_test.to_pickle(dataset_dir + 'test_data_embedding.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def map_test(embedding, index):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return embedding, index\n",
    "\n",
    "\n",
    "def test_dataset_generator(embedding_array, index_array, batch_size):\n",
    "    embedding_list = []\n",
    "    index_list = []\n",
    "    for embedding, index in zip(embedding_array, index_array):\n",
    "        embedding_list.append(embedding)\n",
    "        index_list.append(index)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding_list, index_list))\n",
    "    dataset = dataset.map(map_test, tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "dataset_test = test_dataset_generator(df_test['Embeddings'].values, df_test['ID'].values, hparams['BATCH_SIZE'])\n",
    "print(f'There are {len(df_test):d} samples in testing dataset.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "inference_dir = './inference/'\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def test_step(embedding, noise):\n",
    "    fake_image = generator(embedding, noise, training=False)\n",
    "    return fake_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def inference():\n",
    "    num_test_step = len(df_test) // hparams['BATCH_SIZE']\n",
    "    sample_noise = np.random.normal(loc=0.0, scale=1.0, size=(hparams['Z_DIM'])).astype(np.float32)\n",
    "    sample_noise = np.tile(sample_noise, (hparams['BATCH_SIZE'], 1))\n",
    "\n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for embedding, index in dataset_test:\n",
    "        fake_image = test_step(embedding, sample_noise)\n",
    "        for i in range(hparams['BATCH_SIZE']):\n",
    "            plt.imsave(f'{inference_dir}inference_{index[i]:04d}.jpg', (fake_image[i].numpy() + 1) / 2)\n",
    "\n",
    "        step += 1\n",
    "        if step > num_test_step:\n",
    "            break\n",
    "\n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "inference()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Score & Cosine Similarity\n",
    "\n",
    "**Clear GPU memory to avoid OOM!**\n",
    "\n",
    "Install `numba`\n",
    "```sh\n",
    "$ conda install -c numba numba\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "cuda.get_current_device().reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "score_file = './score.csv'\n",
    "if os.path.exists(score_file):\n",
    "    os.remove(score_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%cd testing\n",
    "!python inception_score_debug.py ../inference/ ../score.csv 39\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if os.path.exists(score_file):\n",
    "    df_score = pd.read_csv(score_file)\n",
    "    mean_score = np.mean(df_score['score'].values)\n",
    "    print(f'Mean Score: {mean_score:f}')\n",
    "else:\n",
    "    print('Evaluation Failed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9936df39123efbb617b59821fa9903b74f50e310a538397c4da2f638e6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
