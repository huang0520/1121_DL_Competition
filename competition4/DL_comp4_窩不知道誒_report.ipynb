{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 4: Recommender Systems\n",
    "\n",
    "Team name: 窩不知道誒\n",
    "\n",
    "Team members: 112501533 黃思誠 112065527 劉承瑋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from evaluation.environment import TestingEnvironment, TrainingEnvironment\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Add, Dot, Embedding, Flatten, Input\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定參數\n",
    "@dataclass\n",
    "class ConstParams:\n",
    "    N_TRAIN_USERS: int = 1000\n",
    "    N_TEST_USERS: int = 2000\n",
    "    N_ITEMS: int = 209527\n",
    "    HORIZON: int = 2000\n",
    "    TEST_EPISODES: int = 5\n",
    "    SLATE_SIZE: int = 5\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HParams:\n",
    "    EMBED_SIZE: int = 128\n",
    "    BATCH_SIZE: int = 128\n",
    "    RANDOM_STATE: int = 42\n",
    "    NUM_EPOCHS: int = 1\n",
    "    NUM_EPOSIDES: int = 5\n",
    "    N_NEGTIVES: int = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    USER_DATA: Path = Path(\"./dataset/user_data.json\")\n",
    "    ITEM_DATA: Path = Path(\"./dataset/item_data.json\")\n",
    "    OUTPUT: Path = Path(\"./output/output.csv\")\n",
    "    CHECKPOINT_DIR: Path = Path(\"./checkpoint\")\n",
    "    TOKEN_PATH: Path = Path(\"./dataset/item_token.pkl\")\n",
    "    EMBEDDING_PATH: Path = Path(\"./dataset/item_to_embedding.pkl\")\n",
    "    USER_DATA_PLUS: Path = Path(\"./dataset/user_data_plus.pkl\")\n",
    "    SIMILARITY_PATH: Path = Path(\"./dataset/similarity_items.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(HParams.RANDOM_STATE)\n",
    "\n",
    "if not Path(\"./dataset\").exists():\n",
    "    Path.mkdir(Path(\"./dataset\"))\n",
    "\n",
    "if not Path(\"./checkpoint\").exists():\n",
    "    Path.mkdir(Path(\"./checkpoint\"))\n",
    "\n",
    "if not Path(\"./output\").exists():\n",
    "    Path.mkdir(Path(\"./output\"))\n",
    "\n",
    "if not Path(\"./checkpoint/FunkSVD\").exists():\n",
    "    Path.mkdir(Path(\"./checkpoint/FunkSVD/\"))\n",
    "\n",
    "if not Path(\"./checkpoint/FunkSVD/best\").exists():\n",
    "    Path.mkdir(Path(\"./checkpoint/FunkSVD/best\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text embeddings\n",
    "\n",
    "由於後續有使用 Content-based 的推薦系統，因此我們先將 item_data 中的 headline、short-description 相連並使用 Sentence transformer 將其轉換為 Embeddings。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Paths.EMBEDDING_PATH.exists():\n",
    "    embedder = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    df_item = pd.read_json(Paths.ITEM_DATA, lines=True)\n",
    "    sentences = df_item[\"headline\"] + \" \" + df_item[\"short_description\"]\n",
    "\n",
    "    embeddings = sentences.apply(lambda x: embedder.encode(x))\n",
    "    embeddings = pd.DataFrame.from_records(embeddings)\n",
    "    embeddings.to_pickle(Paths.EMBEDDING_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset generator, data manager and history manager\n",
    "\n",
    "**Data manager:**\n",
    "- 由於資料量過少，收集更多 User click 的資料\n",
    "- 每次都計算一次 Consine similarity 花太多時間，每計算一次就紀錄 Item 對 Item 的 Cosine similarity 來提昇後續訓練速度\n",
    "- 在 Function 之間傳輸各 Dataframe\n",
    "\n",
    "**History:**\n",
    "- 用於紀錄每一 Epoch Test 過程中的 User click 資料，會在每次 Epoch 之後 Reset\n",
    "\n",
    "**Dataset generator:**\n",
    "- 將前面處理的 Data 轉換成 Tensorflow 的 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, user_path, item_path, token_path, embedding_path):\n",
    "        df_user = pd.read_json(user_path, lines=True)\n",
    "        df_item = pd.read_json(item_path, lines=True)\n",
    "\n",
    "        self.num_users = len(df_user)\n",
    "        self.num_items = len(df_item)\n",
    "\n",
    "        self.pairs = set(df_user.explode(\"history\").itertuples(index=False, name=None))\n",
    "        self.item_to_embedding = pd.read_pickle(embedding_path)\n",
    "\n",
    "        self.pos_item_sets = df_user[\"history\"].apply(set).to_list()\n",
    "        self.similarity_items = {}\n",
    "\n",
    "    def add(self, user_id, item_id):\n",
    "        self.pairs.add((user_id, item_id))\n",
    "        self.pos_item_sets[user_id].add(item_id)\n",
    "\n",
    "    def remove(self, user_id, item_id):\n",
    "        self.pairs.remove((user_id, item_id))\n",
    "        self.pos_item_sets[user_id].discard(item_id)\n",
    "\n",
    "    def get_sequences(self):\n",
    "        return list(self.pairs)\n",
    "\n",
    "    def save(self, user_plus_path, similarity_path):\n",
    "        df_user = pd.DataFrame({\n",
    "            \"user_id\": range(self.num_users),\n",
    "            \"history\": self.pos_item_sets,\n",
    "        })\n",
    "        df_user.to_pickle(user_plus_path)\n",
    "\n",
    "        with Path.open(similarity_path, \"wb\") as f:\n",
    "            pickle.dump(self.similarity_items, f)\n",
    "\n",
    "    def load(self, user_plus_path, similarity_path):\n",
    "        if Path(user_plus_path).exists():\n",
    "            df_user = pd.read_pickle(user_plus_path)\n",
    "            self.pairs = set(\n",
    "                df_user.explode(\"history\").itertuples(index=False, name=None)\n",
    "            )\n",
    "            self.pos_item_sets = df_user[\"history\"].to_list()\n",
    "\n",
    "        if Path(similarity_path).exists():\n",
    "            with Path.open(similarity_path, \"rb\") as f:\n",
    "                self.similarity_items = pickle.load(f)\n",
    "\n",
    "    def add_top100_items(self, item_id, sort_item_ids):\n",
    "        self.similarity_items[item_id] = sort_item_ids[:100]\n",
    "\n",
    "\n",
    "class History:\n",
    "    def __init__(self, user_path):\n",
    "        df_user = pd.read_json(user_path, lines=True)\n",
    "        self.init_histories = df_user.set_index(\"user_id\")[\"history\"]\n",
    "        self.curr_histories = self.init_histories.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_histories = self.init_histories.copy()\n",
    "\n",
    "    def add(self, user_id, item_id):\n",
    "        self.curr_histories.loc[user_id].append(item_id)\n",
    "\n",
    "    def get(self, user_id):\n",
    "        return self.curr_histories.loc[user_id]\n",
    "\n",
    "    def update_init(self, sequence):\n",
    "        self.init_histories = (\n",
    "            pd.DataFrame(sequence, columns=[\"user_id\", \"history\"])\n",
    "            .groupby(\"user_id\")[\"history\"]\n",
    "            .apply(list)\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelDatasetGenerator:\n",
    "    def __init__(self, user_item_pairs):\n",
    "        self.df_seq = pd.DataFrame(user_item_pairs, columns=[\"user_id\", \"item_id\"])\n",
    "        self.df_seq[\"label\"] = 1\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.convert_to_tensor(self.df_seq[\"user_id\"].to_numpy(dtype=int)),\n",
    "            tf.convert_to_tensor(self.df_seq[\"item_id\"].to_numpy(dtype=int)),\n",
    "            tf.convert_to_tensor(self.df_seq[\"label\"].to_numpy(dtype=int)),\n",
    "        ))\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
    "        dataset = dataset.batch(\n",
    "            batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "我們這次分別嘗試了數種推薦系統：\n",
    "- FunkSVD\n",
    "- NeuMF\n",
    "- Factorization Machine\n",
    "- Content-based\n",
    "\n",
    "不過最後是採用了 Content-based + FunkSVD 兩個相對簡單的推薦系統的的組合。其中 FunkSVD 我們加上了 User bias 及 Item bias 來提昇預測準確度。而 Content-based 就是通過 Item 之間的 Embedding 的 Cosine similarity 來找最接近的 Item。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunkSVD(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Simplified Funk-SVD recommender model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_factors, num_users, num_items, l2_lambda=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor of the model\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        # Input\n",
    "        user_id = Input(shape=(1,), dtype=tf.int32)\n",
    "        item_id = Input(shape=(1,), dtype=tf.int32)\n",
    "\n",
    "        # Embedding\n",
    "        vec_user = Embedding(\n",
    "            num_users,\n",
    "            num_factors,\n",
    "            embeddings_initializer=RandomNormal(),\n",
    "            embeddings_regularizer=L2(l2_lambda),\n",
    "        )(user_id)\n",
    "        vec_item = Embedding(\n",
    "            num_items,\n",
    "            num_factors,\n",
    "            embeddings_initializer=RandomNormal(),\n",
    "            embeddings_regularizer=L2(l2_lambda),\n",
    "        )(item_id)\n",
    "        embeddings = Add()([\n",
    "            tf.reduce_sum(Dot(axes=2)([vec_user, vec_item]), axis=2, keepdims=True),\n",
    "        ])\n",
    "\n",
    "        # Bias\n",
    "        b_user = Embedding(\n",
    "            num_users,\n",
    "            1,\n",
    "            embeddings_initializer=RandomNormal(),\n",
    "            embeddings_regularizer=L2(l2_lambda),\n",
    "        )(user_id)\n",
    "        b_item = Embedding(\n",
    "            num_items,\n",
    "            1,\n",
    "            embeddings_initializer=RandomNormal(),\n",
    "            embeddings_regularizer=L2(l2_lambda),\n",
    "        )(item_id)\n",
    "        biases = Add()([\n",
    "            b_user,\n",
    "            b_item,\n",
    "        ])\n",
    "\n",
    "        # Output\n",
    "        output = Add()([embeddings, biases])\n",
    "        output = Flatten()(output)\n",
    "\n",
    "        self.model = keras.Model(\n",
    "            inputs=(\n",
    "                user_id,\n",
    "                item_id,\n",
    "            ),\n",
    "            outputs=output,\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs) -> tf.Tensor:\n",
    "        return self.model(inputs)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        user_ids, item_ids, y_trues = inputs\n",
    "\n",
    "        # compute loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_preds = self.call((user_ids, item_ids))\n",
    "            loss = self.loss(y_trues, y_preds)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def get_topk(self, user_id, k=5) -> tf.Tensor:\n",
    "        user_ids = tf.repeat(tf.constant(user_id), self.num_items)\n",
    "        item_ids = tf.range(self.num_items)\n",
    "        rank_list = tf.squeeze(self.call((user_ids, item_ids)))\n",
    "        return tf.math.top_k(rank_list, k=k).indices.numpy().tolist()\n",
    "\n",
    "\n",
    "def get_content_topk(data_manager, clicked_id, k=2, choose_self=True):\n",
    "    n = 0 if choose_self else 1\n",
    "    if clicked_id in data_manager.similarity_items:\n",
    "        return data_manager.similarity_items[clicked_id][n : n + k]\n",
    "\n",
    "    item_to_embedding = data_manager.item_to_embedding\n",
    "    scores = tf.losses.CosineSimilarity(reduction=\"none\")(\n",
    "        tf.repeat(\n",
    "            tf.constant(item_to_embedding.iloc[clicked_id], shape=(1, 384)),\n",
    "            len(item_to_embedding),\n",
    "            axis=0,\n",
    "        ),\n",
    "        tf.constant(item_to_embedding),\n",
    "    )\n",
    "\n",
    "    sort_items = tf.argsort(scores).numpy().tolist()\n",
    "\n",
    "    data_manager.add_top100_items(clicked_id, sort_items)\n",
    "    return sort_items[n : n + k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train, explore, update function\n",
    "\n",
    "我們將訓練過程分成兩個部份：\n",
    "1. Train: \n",
    "    - 每次 Explore 前會進行數個 Epoch 的 Training，用以讓模型再學習目前全部資料（原本 + 收集）\n",
    "2. Explore: \n",
    "    - 取得 FunkSVD top-2 及 Content-based top-3 的 Item 作為 Slate，其中 Content-based 的方式會是通過隨機選取過去 User click 過的 Item 來作為搜尋的基準\n",
    "    - 如果 User click 那就紀錄並通過 Clicked item 去 Update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, n_neg=14):\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = trange(HParams.NUM_EPOCHS, desc=\"Training\", ncols=0)\n",
    "    for _ in pbar:\n",
    "        batch_loss = []\n",
    "\n",
    "        for user_ids, pos_item_ids, labels in dataset:\n",
    "            losses = []\n",
    "            batch_size = len(user_ids)\n",
    "\n",
    "            # Train positive samples\n",
    "            loss = model.train_step((\n",
    "                user_ids,\n",
    "                pos_item_ids,\n",
    "                labels,\n",
    "            ))\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Train negative samples\n",
    "            neg_item_ids = tf.random.uniform(\n",
    "                shape=(n_neg, batch_size),\n",
    "                minval=0,\n",
    "                maxval=ConstParams.N_ITEMS,\n",
    "                dtype=tf.int32,\n",
    "            )\n",
    "            for _neg_item_id in neg_item_ids:\n",
    "                loss = model.train_step((\n",
    "                    tf.constant(user_ids),\n",
    "                    tf.constant(_neg_item_id),\n",
    "                    tf.zeros(batch_size),\n",
    "                ))\n",
    "                losses.append(loss)\n",
    "\n",
    "            batch_loss.append(tf.reduce_mean(losses).numpy())\n",
    "        epoch_loss.append(np.mean(batch_loss))\n",
    "        pbar.set_postfix({\"loss\": epoch_loss[-1]})\n",
    "    pbar.set_postfix({\"loss\": np.mean(epoch_loss)}, refresh=True)\n",
    "\n",
    "    return model, np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def update(model, user_id, clicked_id):\n",
    "    # Positive samples\n",
    "    model.train_step((\n",
    "        tf.convert_to_tensor([[user_id]]),\n",
    "        tf.convert_to_tensor([[clicked_id]]),\n",
    "        tf.ones(1),\n",
    "    ))\n",
    "\n",
    "    # Negative samples\n",
    "    neg_item_ids = tf.random.uniform(\n",
    "        shape=(HParams.N_NEGTIVES,),\n",
    "        minval=0,\n",
    "        maxval=ConstParams.N_ITEMS,\n",
    "        dtype=tf.int32,\n",
    "    )\n",
    "    model.train_step((\n",
    "        tf.repeat(user_id, HParams.N_NEGTIVES),\n",
    "        neg_item_ids,\n",
    "        tf.zeros(HParams.N_NEGTIVES),\n",
    "    ))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Explore pipeline\n",
    "def explore(env, model, data_manager, slate_size=5):\n",
    "    hit_count = 0\n",
    "    pbar = tqdm(desc=\"Explore\")\n",
    "    while env.has_next_state():\n",
    "        user_id = env.get_state()\n",
    "        random_pos_item_id = random.choice(tuple(data_manager.pos_item_sets[user_id]))\n",
    "        coll_slate = model.get_topk(user_id, 2)\n",
    "        cont_slate = get_content_topk(data_manager, random_pos_item_id, 3, False)\n",
    "        slate = np.unique(coll_slate + cont_slate).tolist()\n",
    "        while len(slate) < slate_size:\n",
    "            slate = np.unique(\n",
    "                slate\n",
    "                + random.sample(model.get_topk(user_id, 10), slate_size - len(slate))\n",
    "            ).tolist()\n",
    "        clicked_id, _ = env.get_response(slate)\n",
    "\n",
    "        if clicked_id != -1:\n",
    "            hit_count += 1\n",
    "            data_manager.add(user_id, clicked_id)\n",
    "            model = update(model, user_id, clicked_id)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"#click\": hit_count})\n",
    "\n",
    "    return model, hit_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Model 的 Optimizer 我們選擇使用 AdamW 並加上其 EMA 的功能來提昇表現，而 Loss function 則是使用 Binary Focal Cross-entropy。其中 Lable smoothing 及 Apply class balanceing 都會讓 model 的表現更好。\n",
    "\n",
    "除此之外，為了避免只拿 Positive sample 訓練可能會表現不好的問題，每訓練一筆 Positive sample 都會從所有 Item 隨機抽幾個 Item 隨機抽幾個作為 Negtive samples 來平衡。 \n",
    "\n",
    "*結果只是示意，非最終訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = DataManager(\n",
    "    Paths.USER_DATA, Paths.ITEM_DATA, Paths.TOKEN_PATH, Paths.EMBEDDING_PATH\n",
    ")\n",
    "data_manager.load(Paths.USER_DATA_PLUS, Paths.SIMILARITY_PATH)\n",
    "\n",
    "model = FunkSVD(\n",
    "    HParams.EMBED_SIZE,\n",
    "    ConstParams.N_TRAIN_USERS,\n",
    "    ConstParams.N_ITEMS,\n",
    "    l2_lambda=0.005,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.0005, weight_decay=0.0004, use_ema=True\n",
    "    ),\n",
    "    loss=tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing=True, from_logits=True, label_smoothing=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, Paths.CHECKPOINT_DIR / \"FunkSVD\", max_to_keep=5\n",
    ")\n",
    "best_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, Paths.CHECKPOINT_DIR / \"FunkSVD/best\", max_to_keep=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Eposide 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 1/1 [00:18<00:00, 18.49s/it, loss=0.102]\n",
      "Explore: 7045it [00:46, 151.14it/s, #click=1392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003523\n",
      "Best model saved at checkpoint/FunkSVD/best/ckpt-2.\n",
      "===== Eposide 2/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 1/1 [00:17<00:00, 17.11s/it, loss=0.0969]\n",
      "Explore: 7016it [00:44, 158.79it/s, #click=1354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003508\n",
      "===== Eposide 3/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 1/1 [00:17<00:00, 17.07s/it, loss=0.0934]\n",
      "Explore: 6934it [00:44, 156.58it/s, #click=1293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003467\n",
      "===== Eposide 4/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 1/1 [00:16<00:00, 16.63s/it, loss=0.0904]\n",
      "Explore: 6928it [00:41, 165.44it/s, #click=1297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003464\n",
      "===== Eposide 5/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 1/1 [00:16<00:00, 16.67s/it, loss=0.0876]\n",
      "Explore: 7077it [00:45, 155.43it/s, #click=1402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Score: 0.003539\n",
      "Best model saved at checkpoint/FunkSVD/best/ckpt-7.\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for i in range(HParams.NUM_EPOSIDES):\n",
    "    print(\"=\" * 5 + f\" Eposide {i + 1}/{HParams.NUM_EPOSIDES} \" + \"=\" * 5)\n",
    "\n",
    "    # Initialize\n",
    "    env = TrainingEnvironment()\n",
    "    dataset_generator = LabelDatasetGenerator(data_manager.get_sequences())\n",
    "\n",
    "    # Train\n",
    "    dataset = dataset_generator(HParams.BATCH_SIZE)\n",
    "    model, _ = train(model, dataset, HParams.N_NEGTIVES)\n",
    "\n",
    "    # Explore and update\n",
    "    model, _ = explore(env, model, data_manager, ConstParams.SLATE_SIZE)\n",
    "    score = np.mean(env.get_score())\n",
    "    print(f\"Avg. Score: {score:.6f}\")\n",
    "\n",
    "    # Save\n",
    "    ckpt_manager.save()\n",
    "    data_manager.save(Paths.USER_DATA_PLUS, Paths.SIMILARITY_PATH)\n",
    "\n",
    "    # Save best model\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_manager.save()\n",
    "        print(f\"Best model saved at {best_manager.latest_checkpoint}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Test 整體的流程跟 Train 很像，只是不會每個 Epoch 之間去 Train，只會在 Explore 的過程中去 Update。而每個 Epoch 後都會重置 Model weight 及紀錄的 History。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from checkpoint/FunkSVD/best_backup/ckpt-88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16990it [02:18, 122.92it/s, #click=4998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from checkpoint/FunkSVD/best_backup/ckpt-88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16988it [02:14, 126.47it/s, #click=4990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from checkpoint/FunkSVD/best_backup/ckpt-88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16907it [02:02, 137.72it/s, #click=4929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from checkpoint/FunkSVD/best_backup/ckpt-88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 16919it [02:07, 133.00it/s, #click=4928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from checkpoint/FunkSVD/best_backup/ckpt-88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 17092it [02:08, 132.78it/s, #click=5066]\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_dir = Paths.CHECKPOINT_DIR / \"FunkSVD/best_backup\"\n",
    "\n",
    "test_env = TestingEnvironment()\n",
    "scores = []\n",
    "\n",
    "# Repeat the testing process for 5 times\n",
    "for epoch in range(ConstParams.TEST_EPISODES):\n",
    "    # [TODO] Load your model weights here (in the beginning of each testing episode)\n",
    "    # [TODO] Code for loading your model weights...\n",
    "    print(f\"Model restored from {tf.train.latest_checkpoint(best_ckpt_dir)}.\")\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(best_ckpt_dir))\n",
    "    history = History(Paths.USER_DATA)\n",
    "    clicked_count = 0\n",
    "\n",
    "    # Start the testing process\n",
    "    with tqdm(desc=\"Testing\") as pbar:\n",
    "        # Run as long as there exist some active users\n",
    "        while test_env.has_next_state():\n",
    "            # Get the current user id\n",
    "            cur_user = test_env.get_state()\n",
    "\n",
    "            # [TODO] Employ your recommendation policy to generate a slate of 5 distinct items\n",
    "            # [TODO] Code for generating the recommended slate...\n",
    "            random_pos_item_id = random.choice(\n",
    "                np.unique(history.get(cur_user)).tolist()\n",
    "            )\n",
    "            coll_slate = model.get_topk(cur_user, 2)\n",
    "            cont_slate = get_content_topk(data_manager, random_pos_item_id, 3, False)\n",
    "            slate = np.unique(coll_slate + cont_slate).tolist()\n",
    "\n",
    "            while len(slate) < ConstParams.SLATE_SIZE:\n",
    "                slate = np.unique(\n",
    "                    slate\n",
    "                    + random.sample(\n",
    "                        model.get_topk(cur_user, 10),\n",
    "                        ConstParams.SLATE_SIZE - len(slate),\n",
    "                    )\n",
    "                ).tolist()\n",
    "\n",
    "            # Get the response of the slate from the environment\n",
    "            clicked_id, _in_environment = test_env.get_response(slate)\n",
    "\n",
    "            # [TODO] Update your model here (optional)\n",
    "            # [TODO] You can update your model at each step, or perform a batched update after some interval\n",
    "            # [TODO] Code for updating your model...\n",
    "            if clicked_id != -1:\n",
    "                clicked_count += 1\n",
    "                history.add(cur_user, clicked_id)\n",
    "                model = update(model, cur_user, clicked_id)\n",
    "                pbar.set_postfix({\"#click\": clicked_count})\n",
    "\n",
    "            # Update the progress indicator\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Record the score of this testing episode\n",
    "    scores.append(test_env.get_score())\n",
    "\n",
    "    # Reset the testing environment\n",
    "    test_env.reset()\n",
    "\n",
    "    # [TODO] Delete or reset your model weights here (in the end of each testing episode)\n",
    "    # [TODO] Code for deleting your model weights...\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(best_ckpt_dir))\n",
    "    history.reset()\n",
    "\n",
    "# Calculate the average scores\n",
    "avg_scores = [np.average(score) for score in zip(*scores)]\n",
    "\n",
    "# Generate a DataFrame to output the result in a .csv file\n",
    "df_result = pd.DataFrame(\n",
    "    [[user_id, avg_score] for user_id, avg_score in enumerate(avg_scores)],\n",
    "    columns=[\"user_id\", \"avg_score\"],\n",
    ")\n",
    "df_result.to_csv(Paths.OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "相比前面三次競賽，我們在這次競賽感覺比較抓不太到方向，即使到最後的表現離 70 分線依然有一段距離。即使有嘗試了許多的方式，但大多連 60 分的門檻都模不到，最後回到簡單的方式反而通過了。而在最後一個小時持續嘗試時才發現 Focal loss 中的 label smooth 的影響很大，但已經來不及再 Train 一個 Model 出來了。\n",
    "\n",
    "不過本次競賽的過程中也學習到很多東西，各種推薦系統的架構，對於一個推薦系統重要的是什麼...等。所以即使分數表現不如預期也不至於太過沮喪。畢竟這緊湊的一個學期總算結束了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
