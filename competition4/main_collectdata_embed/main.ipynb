{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 22:46:29.739641: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 22:46:29.760492: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 22:46:29.760511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 22:46:29.761188: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 22:46:29.765148: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 22:46:30.493843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from evaluation.environment import TrainingEnvironment, TestingEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 22:46:31.416148: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:31.436395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:31.436506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:31.437785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:31.437873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:31.437931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:32.084209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:32.084307: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:32.084370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 22:46:32.084426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:05:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official hyperparameters for this competition (do not modify)\n",
    "N_TRAIN_USERS = 1000\n",
    "N_TEST_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "HORIZON = 2000\n",
    "TEST_EPISODES = 5\n",
    "SLATE_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EMBEDDING_SIZE = 512\n",
    "N_EPOCHS = 2500\n",
    "TRAIN_EPISODES = 50\n",
    "COLLABRATIVE_SLATE_SIZE = 5\n",
    "CONTENT_SLATE_SIZE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "USER_DATA = os.path.join(\"../dataset\", \"user_data.json\")\n",
    "ITEM_DATA = os.path.join(\"../dataset\", \"item_data.json\")\n",
    "EMBEDDINGS_DATA = os.path.join(\"./data\", \"embeddings.json\")\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = os.path.join(\"output\", \"output_main.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[42558, 65272, 13353]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[146057, 195688, 143652]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[67551, 85247, 33714]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[116097, 192703, 103229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[68756, 140123, 135289]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>[95090, 131393, 130239]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>[2360, 147130, 8145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>[99794, 138694, 157888]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>[55561, 60372, 51442]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>[125409, 77906, 124792]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   history\n",
       "0           0     [42558, 65272, 13353]\n",
       "1           1  [146057, 195688, 143652]\n",
       "2           2     [67551, 85247, 33714]\n",
       "3           3  [116097, 192703, 103229]\n",
       "4           4   [68756, 140123, 135289]\n",
       "...       ...                       ...\n",
       "1995     1995   [95090, 131393, 130239]\n",
       "1996     1996      [2360, 147130, 8145]\n",
       "1997     1997   [99794, 138694, 157888]\n",
       "1998     1998     [55561, 60372, 51442]\n",
       "1999     1999   [125409, 77906, 124792]\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user = pd.read_json(USER_DATA, lines=True)\n",
    "df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                           headline  \\\n",
       "0        0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1        1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2        2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3        3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4        4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Health experts said it is too early to predict...  \n",
       "1  He was subdued by passengers and crew when he ...  \n",
       "2  \"Until you have a dog you don't understand wha...  \n",
       "3  \"Accidentally put grown-up toothpaste on my to...  \n",
       "4  Amy Cooper accused investment firm Franklin Te...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item = pd.read_json(ITEM_DATA, lines=True)\n",
    "df_item.head()\n",
    "# df_item[\"headline\"].iloc[df_user.at[0, \"history\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/sclab-nas_home/anaconda3/envs/tf214/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# * create embeddings.json\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPTextModel, AutoTokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_DATA) == False:\n",
    "    output_iter = 0\n",
    "    headlines = df_item[\"headline\"].values.tolist()\n",
    "    descrips = df_item[\"short_description\"].values.tolist()\n",
    "\n",
    "    embedding_file = open('embeddings.json', mode=\"a+\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    for headline, short_description in zip(headlines, descrips):\n",
    "        output_iter += 1\n",
    "        sentences = headline + \" \" + short_description\n",
    "\n",
    "        # Compute embedding for both lists\n",
    "        embedding = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        # util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "        print(embedding.shape)\n",
    "        print(output_iter)\n",
    "        json.dump(embedding.tolist(), embedding_file)\n",
    "        embedding_file.write(os.linesep)\n",
    "    embedding_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034379</td>\n",
       "      <td>0.041217</td>\n",
       "      <td>-0.033026</td>\n",
       "      <td>-0.012551</td>\n",
       "      <td>0.041106</td>\n",
       "      <td>-0.036202</td>\n",
       "      <td>-0.018106</td>\n",
       "      <td>0.112541</td>\n",
       "      <td>-0.019689</td>\n",
       "      <td>-0.014042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010634</td>\n",
       "      <td>-0.023951</td>\n",
       "      <td>-0.034534</td>\n",
       "      <td>-0.066051</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>0.097655</td>\n",
       "      <td>-0.129044</td>\n",
       "      <td>0.032223</td>\n",
       "      <td>0.094735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055801</td>\n",
       "      <td>0.041660</td>\n",
       "      <td>-0.075506</td>\n",
       "      <td>0.014528</td>\n",
       "      <td>0.101326</td>\n",
       "      <td>0.084207</td>\n",
       "      <td>0.107544</td>\n",
       "      <td>-0.058138</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.053167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>-0.040218</td>\n",
       "      <td>0.015820</td>\n",
       "      <td>-0.011099</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>-0.036155</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>0.021352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008935</td>\n",
       "      <td>-0.011113</td>\n",
       "      <td>0.052029</td>\n",
       "      <td>0.062050</td>\n",
       "      <td>0.030391</td>\n",
       "      <td>-0.011244</td>\n",
       "      <td>0.050234</td>\n",
       "      <td>-0.046750</td>\n",
       "      <td>-0.017264</td>\n",
       "      <td>-0.016110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068520</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.026847</td>\n",
       "      <td>-0.058443</td>\n",
       "      <td>0.086685</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>0.100118</td>\n",
       "      <td>0.080488</td>\n",
       "      <td>0.008866</td>\n",
       "      <td>0.006975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055363</td>\n",
       "      <td>-0.013249</td>\n",
       "      <td>-0.002163</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.017737</td>\n",
       "      <td>-0.046325</td>\n",
       "      <td>0.074265</td>\n",
       "      <td>0.045498</td>\n",
       "      <td>-0.014507</td>\n",
       "      <td>-0.010839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>0.021754</td>\n",
       "      <td>0.019326</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.033532</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>0.034945</td>\n",
       "      <td>0.017188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.044316</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>0.033990</td>\n",
       "      <td>0.040945</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.059798</td>\n",
       "      <td>-0.015685</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>-0.015143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003262</td>\n",
       "      <td>-0.091643</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>-0.081777</td>\n",
       "      <td>-0.043873</td>\n",
       "      <td>-0.054822</td>\n",
       "      <td>-0.043604</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>-0.041898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.034379  0.041217 -0.033026 -0.012551  0.041106 -0.036202 -0.018106   \n",
       "1  0.055801  0.041660 -0.075506  0.014528  0.101326  0.084207  0.107544   \n",
       "2  0.008935 -0.011113  0.052029  0.062050  0.030391 -0.011244  0.050234   \n",
       "3 -0.055363 -0.013249 -0.002163  0.003557  0.017737 -0.046325  0.074265   \n",
       "4 -0.044316  0.035937  0.013626  0.033990  0.040945  0.106825  0.059798   \n",
       "\n",
       "        7         8         9    ...       374       375       376       377  \\\n",
       "0  0.112541 -0.019689 -0.014042  ... -0.010634 -0.023951 -0.034534 -0.066051   \n",
       "1 -0.058138  0.020093  0.053167  ...  0.012300 -0.040218  0.015820 -0.011099   \n",
       "2 -0.046750 -0.017264 -0.016110  ...  0.068520  0.007830  0.026847 -0.058443   \n",
       "3  0.045498 -0.014507 -0.010839  ...  0.060975  0.021754  0.019326 -0.001269   \n",
       "4 -0.015685  0.084900 -0.015143  ... -0.003262 -0.091643  0.017924  0.009299   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0  0.010143  0.032896  0.097655 -0.129044  0.032223  0.094735  \n",
       "1 -0.005651 -0.036155 -0.001889  0.000154  0.012841  0.021352  \n",
       "2  0.086685  0.025515  0.100118  0.080488  0.008866  0.006975  \n",
       "3  0.014131  0.025077  0.033532  0.033023  0.034945  0.017188  \n",
       "4 -0.081777 -0.043873 -0.054822 -0.043604  0.024100 -0.041898  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embs = pd.read_json(EMBEDDINGS_DATA, lines=True)\n",
    "df_embs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(209527, 384), dtype=float32, numpy=\n",
       "array([[-0.03437939,  0.04121726, -0.03302589, ..., -0.12904383,\n",
       "         0.03222273,  0.09473488],\n",
       "       [ 0.05580103,  0.04165973, -0.07550589, ...,  0.00015381,\n",
       "         0.01284125,  0.02135223],\n",
       "       [ 0.00893515, -0.01111344,  0.05202927, ...,  0.08048806,\n",
       "         0.00886633,  0.00697471],\n",
       "       ...,\n",
       "       [-0.03284348, -0.05767122,  0.01563493, ...,  0.10245059,\n",
       "         0.0477636 ,  0.10331542],\n",
       "       [-0.12273039,  0.07935417, -0.00262572, ...,  0.00916851,\n",
       "         0.01629837, -0.00017832],\n",
       "       [-0.01579858,  0.03282551, -0.01836753, ..., -0.01433733,\n",
       "        -0.06085267,  0.05207032]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embs = tf.cast(df_embs[:].values, dtype=tf.float32)\n",
    "df_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "\n",
    "\n",
    "def top_k_nearest(sente_id, k):\n",
    "    vec = df_embs[sente_id]\n",
    "\n",
    "    # calaulate cosine similarity  of `vec` and all other vocabularies\n",
    "    dot = np.dot(df_embs.numpy(), vec)\n",
    "    embedding_norm = LA.norm(df_embs.numpy(), axis=-1)\n",
    "    vec_norm = LA.norm(vec)\n",
    "    norm_product = embedding_norm * vec_norm\n",
    "    cos_sim = dot / norm_product\n",
    "\n",
    "    # print out top k nearest words\n",
    "    indices = np.argsort(cos_sim)[::-1][:k]\n",
    "    print(\n",
    "        '---top {} nearest words of {}. {}---'.format(\n",
    "            k, sente_id, df_item.at[sente_id, \"headline\"]\n",
    "        )\n",
    "    )\n",
    "    for idx in indices:\n",
    "        print(f\"{idx}. {df_item.at[idx, 'headline']}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---top 6 nearest words of 0. Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters---\n",
      "0. Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "5224. U.S. Coronavirus Death Toll Climbs To 6, All In Washington State\n",
      "5188. Nearly 1,000 People Diagnosed With Coronavirus In U.S.\n",
      "1497. Fauci Says Those Who Want To Be 'Optimally Protected' Should Get COVID-19 Booster\n",
      "3312. Biden Administration Says It Will Buy 200 Million More COVID-19 Shots\n",
      "1543. Top Health Officials Share Optimism, Concern On Omicron Variant Of COVID\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_nearest(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "train_data = []\n",
    "\n",
    "histories = []\n",
    "uids = []\n",
    "ratings = []\n",
    "\n",
    "for uid in df_user.user_id:\n",
    "    for his in df_user.at[uid, \"history\"]:\n",
    "        uids.append(uid)\n",
    "        histories.append(his)\n",
    "        ratings.append(1)\n",
    "uids = tf.convert_to_tensor(uids, dtype=tf.float32)\n",
    "histories = tf.convert_to_tensor(histories, dtype=tf.float32)\n",
    "ratings = tf.convert_to_tensor(ratings, dtype=tf.float32)\n",
    "# print(type(uids))\n",
    "# print(df_user.at[uid, \"history\"])\n",
    "# df_user.at[uid, \"history\"].append(55)\n",
    "# print(df_user.at[uid, \"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# Collabrative Model: funk-svd\n",
    "class FunkSVDRecommender(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Simplified Funk-SVD recommender model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m_users: int, n_items: int, embedding_size: int, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Constructor of the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.m = m_users\n",
    "        self.n = n_items\n",
    "        self.k = embedding_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # user embeddings P\n",
    "        self.P = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.m, self.k)))\n",
    "\n",
    "        # item embeddings Q\n",
    "        self.Q = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.n, self.k)))\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.optimizer_update = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, user_ids: tf.Tensor, item_ids: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass used in training and validating\n",
    "        \"\"\"\n",
    "        # dot product the user and item embeddings corresponding to the observed interaction pairs to produce predictions\n",
    "        y_pred = tf.reduce_sum(\n",
    "            tf.gather(self.P, indices=user_ids) * tf.gather(self.Q, indices=item_ids), axis=1\n",
    "        )\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the MSE loss of the model\n",
    "        \"\"\"\n",
    "        loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, uids: tf.Tensor, histories: tf.Tensor, ratings: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Train the model with one batch\n",
    "        data: batched user-item interactions\n",
    "        each record in data is in the format [UserID, MovieID, Rating, Timestamp]\n",
    "        \"\"\"\n",
    "        print(\"train\")\n",
    "        user_ids = tf.cast(uids, dtype=tf.int32)\n",
    "        item_ids = tf.cast(histories, dtype=tf.int32)\n",
    "        y_true = tf.cast(ratings, dtype=tf.float32)\n",
    "\n",
    "        print(f\"uid:{user_ids} items{item_ids} y: {y_true}\")\n",
    "\n",
    "        # compute loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(user_ids, item_ids)\n",
    "            loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(self, uids: tf.Tensor, histories: tf.Tensor, ratings: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Validate the model with one batch\n",
    "        data: batched user-item interactions\n",
    "        each record in data is in the format [UserID, MovieID, Rating, Timestamp]\n",
    "        \"\"\"\n",
    "        user_ids = tf.cast(uids, dtype=tf.int32)\n",
    "        item_ids = tf.cast(histories, dtype=tf.int32)\n",
    "        y_true = tf.cast(ratings, dtype=tf.float32)\n",
    "\n",
    "        # compute loss\n",
    "        y_pred = self(user_ids, item_ids)\n",
    "        loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def eval_predict_onestep(self, query: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Retrieve and return the MovieIDs of the 10 recommended movies given a query\n",
    "        You should return a tf.Tensor with shape=(10,)\n",
    "        query will be a tf.Tensor with shape=(2,) and dtype=tf.int64\n",
    "        query[0] is the UserID of the query\n",
    "        #### query[1] is the Timestamp of the query\n",
    "        \"\"\"\n",
    "        # dot product the selected user and all item embeddings to produce predictions\n",
    "        user_id = tf.cast(query, tf.int32)\n",
    "        y_pred = tf.reduce_sum(tf.gather(self.P, user_id) * self.Q, axis=1)\n",
    "\n",
    "        # select the top 10 items with highest scores in y_pred\n",
    "        y_recommends = tf.math.top_k(y_pred, k=COLLABRATIVE_SLATE_SIZE).indices\n",
    "\n",
    "        return y_recommends\n",
    "\n",
    "    @tf.function\n",
    "    def eval_update_onestep(\n",
    "        self, uids: tf.Tensor, histories: tf.Tensor, ratings: tf.Tensor\n",
    "    ) -> None:\n",
    "        # data = data[None, :]  # add a dim on axis 0\n",
    "        # user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "        # item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "        # y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "        user_ids = tf.cast(uids, dtype=tf.int32)\n",
    "        item_ids = tf.cast(histories, dtype=tf.int32)\n",
    "        y_true = tf.cast(ratings, dtype=tf.float32)\n",
    "\n",
    "        # compute loss\n",
    "        y_pred = self(user_ids, item_ids)\n",
    "        loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(user_ids, item_ids)\n",
    "            loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer_update.apply_gradients(zip(gradients, self.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# selected_slate = [[]] * N_TEST_USERS\n",
    "# print(len(uids))\n",
    "# print(len(histories))\n",
    "# for i in range(len(uids)):\n",
    "#     u = int(uids[i])\n",
    "#     print(u)\n",
    "#     # for hi in range(len(histories)):\n",
    "#     #     h = histories[hi]\n",
    "#     #     # print(h)\n",
    "#     #     selected_slate[u].append(float(h))\n",
    "#     print(selected_slate[u])\n",
    "#     selected_slate[u].append(float(histories[i]))\n",
    "# print(\"done\")\n",
    "# selected_slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "model = FunkSVDRecommender(\n",
    "    m_users=N_TEST_USERS,\n",
    "    n_items=N_ITEMS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "\n",
    "    # training\n",
    "    # for data in tqdm(uids, desc='Training'):\n",
    "    loss = model.train_step(uids, histories, ratings)\n",
    "    train_loss.append(loss.numpy())\n",
    "\n",
    "    # # validating\n",
    "    # for data in tqdm(dataset_val, desc='Validating'):\n",
    "    loss = model.val_step(uids, histories, ratings)\n",
    "    val_loss.append(loss.numpy())\n",
    "\n",
    "    # record losses\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    # train_losses.append(avg_train_loss)\n",
    "    # val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print losses\n",
    "    print(f\"Epoch {epoch} train_loss: {avg_train_loss:.4f}, val_loss: {avg_val_loss:.4f}\\n\")\n",
    "model.save(\"model_funk_svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# Initialize the training environment\n",
    "train_env = TrainingEnvironment()\n",
    "\n",
    "# selected_slate = [[]] * N_TEST_USERS\n",
    "# for i in range(uids):\n",
    "#     u = uids[i]\n",
    "#     selected_slate[u].append(histories[i])\n",
    "\n",
    "for _ in range(TRAIN_EPISODES):\n",
    "    # Reset the training environment (this can be useful when you have finished one episode of simulation and do not want to re-initialize a new environment)\n",
    "    train_env.reset()\n",
    "\n",
    "    # Check if there exist any active users in the environment\n",
    "    # env_has_next_state = train_env.has_next_state()\n",
    "    while train_env.has_next_state():\n",
    "        # print(f'There is {\"still some\" if env_has_next_state else \"no\"} active users in the training environment.')\n",
    "\n",
    "        # Get the current user ID\n",
    "        user_id = train_env.get_state()\n",
    "        print(f'The current user is user {user_id}.')\n",
    "\n",
    "        # Get the response of recommending the slate to the current user\n",
    "        slate = model.eval_predict_onestep([user_id])\n",
    "        # user_first_his = df_user.at[user_id, 'history'][0]\n",
    "        # content_slate = get_recommendations(user_first_his)\n",
    "        # print(type(content_slate))\n",
    "\n",
    "        # slate = content_slate.append(collab_slate)\n",
    "\n",
    "        clicked_id, in_environment = train_env.get_response(slate)\n",
    "        uids_ = []\n",
    "        histories_ = []\n",
    "        ratings_ = []\n",
    "        for his in df_user.at[user_id, \"history\"]:\n",
    "            uids_.append(user_id)\n",
    "            histories_.append(his)\n",
    "            ratings_.append(1)\n",
    "        for s in slate:\n",
    "            uids_.append(user_id)\n",
    "            histories_.append(s)\n",
    "            if clicked_id == s:\n",
    "                ratings_.append(1)\n",
    "                df_user.at[uid, \"history\"].append(s)\n",
    "            else:\n",
    "                ratings_.append(-1)\n",
    "        model.eval_update_onestep(uids_, histories_, ratings_)\n",
    "        if clicked_id != -1:\n",
    "            print(\n",
    "                f'The click result of recommending {slate} to user {user_id} is {f\"item {clicked_id}\" if clicked_id != -1 else f\"{clicked_id} (no click)\"}.'\n",
    "            )\n",
    "            print(\n",
    "                f'User {user_id} {\"is still in\" if in_environment else \"leaves\"} the environment.'\n",
    "            )\n",
    "            print(ratings_)\n",
    "\n",
    "model.save(\"model_funk_svd\")\n",
    "\n",
    "# Get the normalized session length score of all users\n",
    "train_score = train_env.get_score()\n",
    "df_train_score = pd.DataFrame(\n",
    "    [[user_id, score] for user_id, score in enumerate(train_score)],\n",
    "    columns=[\"user_id\", \"avg_score\"],\n",
    ")\n",
    "df_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# Initialize the testing environment\n",
    "test_env = TestingEnvironment()\n",
    "scores = []\n",
    "\n",
    "# The item_ids here is for the random recommender\n",
    "# item_ids = [i for i in range(N_ITEMS)]\n",
    "\n",
    "# Repeat the testing process for 5 times\n",
    "for _ in range(TEST_EPISODES):\n",
    "    # [TODO] Load your model weights here (in the beginning of each testing episode)\n",
    "    # [TODO] Code for loading your model weights...\n",
    "    # model.save('model_funk_svd_update')\n",
    "    # model = tf.keras.models.load_model('model_funk_svd', compile=False)\n",
    "\n",
    "    # Start the testing process\n",
    "    with tqdm(desc=\"Testing\") as pbar:\n",
    "        # Run as long as there exist some active users\n",
    "        while test_env.has_next_state():\n",
    "            # Get the current user id\n",
    "            cur_user = test_env.get_state()\n",
    "\n",
    "            # [TODO] Employ your recommendation policy to generate a slate of 5 distinct items\n",
    "            # [TODO] Code for generating the recommended slate...\n",
    "            # Here we provide a simple random implementation\n",
    "            # slate = random.sample(item_ids, k=SLATE_SIZE)\n",
    "            # model = tf.keras.models.load_model('model_funk_svd_update')\n",
    "            # print(cur_user)\n",
    "            collab_slate = model.eval_predict_onestep([cur_user])\n",
    "            # content_slate = get_recommendations(df_user[0])\n",
    "\n",
    "            # Get the response of the slate from the environment\n",
    "            clicked_id, in_environment = test_env.get_response(slate)\n",
    "\n",
    "            # [TODO] Update your model here (optional)\n",
    "            # [TODO] You can update your model at each step, or perform a batched update after some interval\n",
    "            # [TODO] Code for updating your model...\n",
    "\n",
    "            uids_ = []\n",
    "            histories_ = []\n",
    "            ratings_ = []\n",
    "            for his in df_user.at[cur_user, \"history\"]:\n",
    "                uids_.append(cur_user)\n",
    "                histories_.append(his)\n",
    "                ratings_.append(1)\n",
    "            for s in slate:\n",
    "                uids_.append(cur_user)\n",
    "                histories_.append(s)\n",
    "                if clicked_id == s:\n",
    "                    ratings_.append(1)\n",
    "                    df_user.at[uid, \"history\"].append(s)\n",
    "                else:\n",
    "                    ratings_.append(-1)\n",
    "\n",
    "            model.eval_update_onestep(uids_, histories_, ratings_)\n",
    "            # model.save('model_funk_svd_update')\n",
    "\n",
    "            # Update the progress indicator\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Record the score of this testing episode\n",
    "    scores.append(test_env.get_score())\n",
    "\n",
    "    # Reset the testing environment\n",
    "    test_env.reset()\n",
    "\n",
    "    # [TODO] Delete or reset your model weights here (in the end of each testing episode)\n",
    "    # [TODO] Code for deleting your model weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# Calculate the average scores\n",
    "avg_scores = [np.average(score) for score in zip(*scores)]\n",
    "\n",
    "# Generate a DataFrame to output the result in a .csv file\n",
    "df_result = pd.DataFrame(\n",
    "    [[user_id, avg_score] for user_id, avg_score in enumerate(avg_scores)],\n",
    "    columns=[\"user_id\", \"avg_score\"],\n",
    ")\n",
    "df_result.to_csv(OUTPUT_PATH, index=False)\n",
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
